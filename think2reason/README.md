<h1 align="center">Awesome Thinking with PI (<u>P</u>erception & <u>I</u>nteraction)</h1>

<p align="center">
  <b>A curated list of resources on visual reasoning, video understanding, embodied AI, robot action, and perception-driven interaction.</b>
</p>

<!-- Top badges -->
<p align="center">
  <a href="https://github.com/2-mo/Awesome-Thinking-with-PI/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-MIT-blueviolet" alt="License: MIT"></a>
  <a href="https://github.com/2-mo/Awesome-Thinking-with-PI/issues"><img src="https://img.shields.io/badge/Issues-Track-orange" alt="Issues"></a>
  <a href="https://github.com/2-mo/Awesome-Thinking-with-PI/pulls"><img src="https://img.shields.io/badge/Pull%20Requests-Welcome-brightgreen" alt="Pull Requests"></a>
  <a href="https://github.com/2-mo/Awesome-Thinking-with-PI/commits/main"><img src="https://img.shields.io/badge/Commits-Main-blue" alt="Commits: main"></a>
</p>



## ğŸ“š Contents

- ğŸ¤” [Why We Need Thinking?](#why-we-need-thinking)
- ğŸ’­ [Thinking with Language](#thinking-with-language-symbolic-level)
  - CoT / ToT / GoT
  - r1-like reasoning models
- ğŸŒ‰ [Thinking across Modalities](#thinking-across-modalities-multimodal-level)
  - PPO / DPO / GRPO
  - RLHF for multimodal reasoning
- ğŸ” [Thinking with Visual Operations](#thinking-with-visual-operations-interactive-level)
  - GUI interaction / screen control / visual grounding
- ğŸŒ [Thinking in the Physical World](#thinking-in-the-physical-world-embodied-level)
  - Robotics / embodied navigation / manipulation
- ğŸ› ï¸ [Tutorials and Tooling](#tutorials-and-tooling)
- ğŸ“– [Related Collections](#related-collections)

---

## ğŸ¤” Why We Need Thinking?

æ— è®ºåœ¨äººç±»è§†è§‰è¿˜æ˜¯å¤šæ¨¡æ€æ¨¡å‹é‡Œï¼Œæ„ŸçŸ¥ç»™å‡ºçš„è§‚æµ‹å¾€å¾€ä¸å®Œå…¨ã€å«å™ªä¸”å¤šè§£ï¼Œå¯é å†³ç­–å¿…é¡»ä¾èµ–è·¨æ—¶æ•´åˆä¸å‡è®¾æ£€éªŒâ€”â€”è¿™å°±æ˜¯â€œæ€è€ƒâ€ã€‚

### äººç±»è§†è§‰çš„å¯ç¤º

- è¾“å…¥ä¸å®Œæ•´ï¼šç¬æ—¶æ„ŸçŸ¥é›¶ç¢ï¼ŒäºŒç»´åˆ°ä¸‰ç»´å­˜åœ¨å¤©ç„¶æ­§ä¹‰ï¼Œä»…é ç›´æ¥æ„ŸçŸ¥å®¹æ˜“è¢«é”™è§‰ä¸é®æŒ¡è¯¯å¯¼ã€‚
- é¢„æµ‹â€”æ ¡æ­£å¾ªç¯ï¼šè§†è§‰ä¾èµ–è‡ªä¸Šè€Œä¸‹ä¸è‡ªä¸‹è€Œä¸Šçš„äº’åŠ¨ï¼Œé€šè¿‡å‡è®¾ç”Ÿæˆã€è¯¯å·®ä¿®æ­£æ¥æŠµå¾¡æ­§ä¹‰ã€‚
- è·¨æ—¶ä¸ä¸»åŠ¨æ§åˆ¶ï¼šç¨³å¥è¡Œä¸ºä¾èµ–è·¨æ—¶å› æœè¿½è¸ªã€é€Ÿåº¦â€“å‡†ç¡®æ€§æƒè¡¡ï¼Œä»¥åŠç›®æ ‡é©±åŠ¨çš„æ³¨æ„ä¸èµ„æºåˆ†é…ã€‚

### å¯¹åº”åˆ°å¤šæ¨¡æ€å¤§æ¨¡å‹

- å¤šæ¨¡æ€è¾“å…¥åŒæ ·ä¸å…¨ï¼šå›¾åƒå¯èƒ½é®æŒ¡ï¼Œè¯­éŸ³å«å™ªï¼Œæ–‡æœ¬æ­§ä¹‰ï¼Œæ¨¡æ€é—´è¿˜å¯èƒ½äº’ç›¸å†²çªã€‚
- å•æ¬¡ååº”æ˜“åå·®ï¼šåªä¾èµ–â€œå¿«æ„ŸçŸ¥â€å®¹æ˜“è¢«å±€éƒ¨æˆ–é”™è¯¯çº¿ç´¢ç‰µå¼•ã€‚
- æ€è€ƒæ‰èƒ½ç¨³å¥ï¼šè·¨æ¨¡æ€/è·¨æ—¶é—´æ•´åˆï¼Œå‡è®¾æ¯”è¾ƒä¸æ£€éªŒï¼Œé£é™©ä¸‹è‡ªé€‚åº”è°ƒæ§ï¼Œæ‰èƒ½å°†å˜ˆæ‚ä¸å…¨çš„æ„ŸçŸ¥è½¬åŒ–ä¸ºå¯é å†³ç­–ã€‚

### å¯¹åº”åˆ°è§†é¢‘å¼‚å¸¸æ£€æµ‹

å‚è§ä¸“é¢˜æ•´ç†ï¼š[LLM4VAD Â· Video Anomaly Detection](../llm4vad/README.md)

ä¸Šä¸‹æ–‡ä¾èµ–ï¼ˆå¤æ‚æ€§ï¼‰ï¼šå¼‚å¸¸å¾€å¾€æ˜¯é•¿æ—¶åºäº‹ä»¶ï¼ˆæ‰“æ–—ã€äº‹æ•…ï¼‰ï¼Œéœ€è¦ç»“åˆå‰åå› æœä¸åœºæ™¯å…³ç³»æ‰èƒ½æ­£ç¡®åˆ¤å®šã€‚

æ­§ä¹‰æ··æ·†ï¼ˆæ¨¡ç³Šæ€§ï¼‰ï¼šå±€éƒ¨è¡Œä¸ºæˆ–åœºæ™¯å®¹æ˜“ä¸å¼‚å¸¸æ··æ·†ï¼ˆå¥”è·‘ vs é€ƒè·‘ã€èšé›† vs æš´ä¹±ï¼‰ï¼Œå¿…é¡»é€šè¿‡æ›´é•¿æ—¶åºå’Œå¤šæ¨¡æ€çº¿ç´¢æ¥æ¶ˆè§£ã€‚

é•¿å°¾åˆ†å¸ƒï¼ˆç¨€ç–æ€§ï¼‰ï¼šå¼‚å¸¸åœ¨è§†é¢‘æµä¸­å‡ºç°é¢‘ç‡æä½ã€æ—¶æœºä¸å¯æ§ï¼Œå•æ¬¡è§‚æµ‹æ˜“æ¼æ£€ï¼Œå¿…é¡»è·¨æ—¶ç´¯ç§¯è¯æ®ä¸å‡è®¾æ£€éªŒã€‚



#### å…¶å®â€œæ€è€ƒâ€å¹¶ä¸æ˜¯åªåœ¨å¼‚å¸¸åœºæ™¯é‡Œæ‰éœ€è¦ï¼Œè€Œæ˜¯åœ¨å¼‚å¸¸é—®é¢˜ä¸Šï¼Œå®ƒçš„å¿…è¦æ€§è¢«æ”¾å¤§ï¼š

å¸¸æ€æ¨¡å¼å®¹æ˜“é æ„ŸçŸ¥è§£å†³ï¼šæ­£å¸¸è¡Œä¸º/åœºæ™¯å æ®ç»å¤§å¤šæ•°ï¼Œè§„å¾‹æ€§å¼ºã€æ•°æ®é‡å¤§ï¼Œå•é æ„ŸçŸ¥æ¨¡å¼åŒ¹é…å°±èƒ½è¾¾åˆ°ä¸é”™çš„æ•ˆæœã€‚

å¼‚å¸¸æœ¬è´¨ä¸Šæ˜¯â€œä¸ç¡®å®šâ€ï¼šå¼‚å¸¸å¾€å¾€ç¨€ç–ã€å°‘æ ·æœ¬ï¼Œç¼ºä¹å…ˆéªŒç»Ÿè®¡æ”¯æ’‘ã€‚ä»…é å¿«é€Ÿæ„ŸçŸ¥ä¼šå‡ºç°åå·®ï¼Œéœ€è¦è·¨æ—¶æ•´åˆå’Œå‡è®¾æ£€éªŒæ¥å¼¥è¡¥ã€‚

å¼‚å¸¸æ¶‰åŠæ›´å¤§é£é™©ï¼šä¸€æ—¦è¯¯åˆ¤ï¼Œå¯èƒ½å¸¦æ¥ä¸¥é‡åæœï¼ˆæ¼æŠ¥å®‰å…¨äº‹ä»¶ã€è¯¯æŠ¥å¹²æ‰°ç³»ç»Ÿï¼‰ï¼Œå› æ­¤å¿…é¡»å¼•å…¥æ›´æ…¢ã€æ›´ç¨³å¥çš„å†³ç­–æœºåˆ¶ã€‚

å¼‚å¸¸å¾€å¾€æ‰“ç ´å¸¸è§„ï¼šå®ƒä»¬å¯èƒ½è¡¨ç°ä¸ºå¤æ‚çš„ä¸Šä¸‹æ–‡ä¾èµ–ã€æ¨¡ç³Šçš„è¯­ä¹‰æ··æ·†ã€é•¿å°¾çš„ç¨€ç–åˆ†å¸ƒâ€”â€”è¿™äº›éƒ½æ°å¥½æ˜¯â€œæ€è€ƒâ€æ“…é•¿å¤„ç†çš„ã€‚


æˆ‘ä»¬éœ€è¦çš„æ˜¯æ¨ç†ï¼Œè€Œä¸ä»…æ˜¯äº‹åè§£é‡Šã€‚



[![Wiki](https://img.shields.io/badge/Wiki-Thinking%2C%20Fast%20and%20Slow-blue?logo=wikipedia)](https://en.wikipedia.org/wiki/Thinking,_Fast,_and_Slow)


æ³¨ï¼šå¯ä¿¡å¹¶éä»…æ¥è‡ªâ€œå¯è§£é‡Šæ€§â€ï¼Œè€Œæ˜¯æ¥è‡ªé•¿æœŸè®­ç»ƒä¸çœŸå®ä¸–ç•Œçš„ç¨³å®šè¡¨ç°ï¼ˆå‚è§ä¸€æ¬¡æ¼”è®²ä¸­çš„æ¯”å–»ï¼šæˆ‘ä»¬ä¿¡ä»»é™Œç”Ÿå¸æœºï¼Œå¤šå› å¯é ç»éªŒè€Œéå®Œå…¨å¯è§£é‡Šçš„å¤§è„‘æœºç†ï¼‰ã€‚

Why Would You Trust the Human Driver?

> Paraphrase from the [[talk]](https://www.youtube.com/watch?v=NA6EH8r-IT0): In response to a question about interpretability, Kaiming He asksâ€”why do you trust a taxi driver you don't know? Not because the brain is fully interpretable, but because extensive realâ€‘world training and testing make performance reliable; just like airplanes are trusted after millions of flights. Interpretability matters, yet reliability is ultimately earned through empirical evidence.



### åŸºæœ¬æ¦‚å¿µ

Similar to how a human may think for a long time before responding to a difficult question, o1 uses a chain of thought when attempting to solve a problem. Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses. It learns to recognize and correct its mistakes. It learns to break down tricky steps into simpler ones. It learns to try a different approach when the current one isnâ€™t working. This process dramatically improves the modelâ€™s ability to reason.
ä¸äººç±»åœ¨å›ç­”éš¾é¢˜ä¹‹å‰å¯èƒ½ä¼šæ€è€ƒå¾ˆé•¿æ—¶é—´ç±»ä¼¼ï¼Œo1 åœ¨å°è¯•è§£å†³é—®é¢˜æ—¶ä¹Ÿä¼šä½¿ç”¨æ€ç»´é“¾ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œo1 å¯ä»¥å­¦ä¼šç£¨ç»ƒè‡ªå·±çš„æ€ç»´é“¾ï¼Œå¹¶å®Œå–„è‡ªå·±ä½¿ç”¨çš„ç­–ç•¥ã€‚å®ƒå­¦ä¼šè¯†åˆ«å’Œçº æ­£é”™è¯¯ã€‚å®ƒå­¦ä¼šæŠŠæ£˜æ‰‹çš„æ­¥éª¤åˆ†è§£æˆæ›´ç®€å•çš„æ­¥éª¤ã€‚å®ƒå­¦ä¼šåœ¨å½“å‰æ–¹æ³•æ— æ•ˆæ—¶å°è¯•ä¸åŒçš„æ–¹æ³•ã€‚è¿™ä¸€è¿‡ç¨‹æå¤§åœ°æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

åˆ©ç”¨ LLM å­¸ç¿’æ¨ç† â€” [OpenAI: Learning to Reason with LLMs](https://openai.com/zh-Hant/index/learning-to-reason-with-llms/)

Thinking with Images â€” [OpenAI](https://openai.com/index/thinking-with-images/)



[![OpenAI o1](https://img.shields.io/badge/OpenAI-ChatGPT--o1-9cf?logo=openai)](https://openai.com/o1/)
[![DeepSeek-R1](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1?style=social&label=DeepSeek-R1&logo=github)](https://github.com/deepseek-ai/DeepSeek-R1)

### å‚è€ƒä¸ç†è®ºæ”¯æ’‘

- Wang Yifei et al., A Theoretical Understanding of Self-Correction through In-context Alignment, NeurIPS 2024.
- Zhang Zhuosheng et al., Automatic Chain of Thought Prompting in Large Language Models, ICLR 2023.
- Zhao Andrew et al., ExpeL: LLM Agents Are Experiential Learners, AAAI 2024.



---


## ğŸ’­ Thinking with Language (Symbolic-level)

ä¸€å¥è¯æ¦‚è§ˆï¼šä»¥â€œè¯­è¨€â€ä¸ºæ˜¾å¼ä¸­é—´å±‚ï¼ŒæŠŠå¤æ‚ä»»åŠ¡æ‹†æˆæ­¥éª¤â†’åœ¨æ€ç»´ç©ºé—´æœç´¢â†’è°ƒç”¨å·¥å…·æ ¡éªŒâ†’ç”¨è¯„å®¡ä¸è¿‡ç¨‹ç›‘ç£æŒç»­æ”¹è¿›ã€‚

> Quick recipe: CoT â†’ Self-Consistency/ToT â†’ ReAct+Tools â†’ Judge/Refine â†’ PRM/DPO.


2022ï¼šè¯­è¨€æˆä¸ºâ€œå¯è§æ€ç»´â€

Letâ€™s Verify Step by Step (process supervision/PRM): [arXiv:2305.20050](https://arxiv.org/abs/2305.20050)

- Chain-of-Thoughtï¼ˆCoTï¼‰ï¼šç¤ºä¾‹å¼•å¯¼é€æ­¥æ¨ç† â€” Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)
- Self-Consistencyï¼šå¤šæ ·åŒ–æ€è·¯æŠ•ç¥¨ â€” Self-Consistency Improves Chain of Thought Reasoning in Language Models [arXiv:2203.11171](https://arxiv.org/abs/2203.11171)
- STaRï¼šç”¨æ¨¡å‹ç”Ÿæˆçš„æ¨ç†é“¾åè’¸é¦è®­ç»ƒ â€” STaR: Bootstrapping Reasoning with Reasoning [arXiv:2203.14465](https://arxiv.org/abs/2203.14465)
- Least-to-Mostï¼šå…ˆåˆ†è§£å†æ±‚è§£ â€” Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [arXiv:2210.00720](https://arxiv.org/abs/2210.00720)
- Program-of-Thoughtï¼šæŠŠæ€ç»´è½¬ä¸ºå¯æ‰§è¡Œç¨‹åº â€” Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks [arXiv:2211.12588](https://arxiv.org/abs/2211.12588)


2023ï¼šä»â€œå†™å‡ºæ€è·¯â€åˆ°â€œæœç´¢ä¸æ‰§è¡Œâ€

- ReActï¼šæ€ç»´+è¡ŒåŠ¨çš„äº¤æ›¿ï¼ˆæ£€ç´¢/å·¥å…·ä½¿ç”¨ï¼‰ â€” ReAct: Synergizing Reasoning and Acting in Language Models [arXiv:2210.03629](https://arxiv.org/abs/2210.03629)
- Toolformer/Function Callingï¼šè‡ªåŠ¨å­¦ä¹ ä½•æ—¶ç”¨å·¥å…· â€” Toolformer: Language Models Can Teach Themselves to Use Tools [arXiv:2302.04761](https://arxiv.org/abs/2302.04761)
- PAL/PoTï¼šå°†æ€ç»´è½¬ä¸ºå¯æ‰§è¡Œç¨‹åº/ä»£ç æ±‚è§£ â€” PAL: Program-aided Language Models [arXiv:2211.10435](https://arxiv.org/abs/2211.10435)ï¼›Program of Thoughts Prompting [arXiv:2211.12588](https://arxiv.org/abs/2211.12588)
- Tree/Graph of Thoughtsï¼šåœ¨æ€ç»´ç©ºé—´è¿›è¡Œæ ‘/å›¾æœç´¢ä¸è¯„ä¼° â€” Tree of Thoughts [arXiv:2305.10601](https://arxiv.org/abs/2305.10601)ï¼›Graph of Thoughts [arXiv:2308.09687](https://arxiv.org/abs/2308.09687)
- Self-Refine/Reflexionï¼šè‡ªæˆ‘åé¦ˆä¸åæ€æ”¹è¿› â€” Self-Refine [arXiv:2303.17651](https://arxiv.org/abs/2303.17651)ï¼›Reflexion [arXiv:2303.11366](https://arxiv.org/abs/2303.11366)
- LLM-as-a-Judgeï¼šç”¨æ¨¡å‹è¯„å®¡æ¨¡å‹è¾“å‡ºï¼Œæ”¯æ’‘è‡ªç›‘ç£ä¸å¯¹æ¯”ä¼˜åŒ–ï¼ˆDPOï¼‰ â€” LLM-as-a-Judge [arXiv:2306.05685](https://arxiv.org/abs/2306.05685)ï¼›DPO [arXiv:2305.18290](https://arxiv.org/abs/2305.18290)

*æ³¨ï¼šTree/Graph of Thoughtsã€MCTSã€ReAct äº¤äº’å¼æœç´¢ã€‚*

2024ï¼šé¢å‘â€œè¿‡ç¨‹è´¨é‡â€çš„è®­ç»ƒä¸éªŒè¯

- è¿‡ç¨‹ç›‘ç£ä¸PRMï¼ˆProcess Reward Modelï¼‰ï¼šå¥–åŠ±â€œæ€è·¯è¿‡ç¨‹â€çš„æ­£ç¡®æ€§ â€” ä»£è¡¨ä½œï¼šLetâ€™s Verify Step by Step [arXiv:2305.20050](https://arxiv.org/abs/2305.20050)
- RLAIF/åˆæˆåé¦ˆï¼šç”¨AIæˆ–è§„åˆ™æä¾›å¯¹é½ä¿¡å· â€” Constitutional AI: Harmlessness from AI Feedback [arXiv:2212.08073](https://arxiv.org/abs/2212.08073)
- å¼ºåŒ–â€œæ€ç»´â€”æœç´¢â€”éªŒè¯â€é—­ç¯ï¼šæŠŠCoTä¸MCTS/å·¥å…·è°ƒç”¨/æ ¡éªŒå™¨ç»“åˆ
- DSPy/å¯ç¼–æ’æ€ç»´ï¼šå£°æ˜å¼åœ°ç»„åˆæ¨ç†æ¨¡å—ä¸æ£€ç´¢/å·¥å…· â€” [GitHub: stanfordnlp/dspy](https://github.com/stanfordnlp/dspy)


2025ï¼š

- Supervisionï¼ˆç›‘ç£/è®­ç»ƒï¼‰ï¼šä»ç­”æ¡ˆç›‘ç£èµ°å‘è¿‡ç¨‹ç›‘ç£ï¼ˆSTaRã€DPOã€PRMã€RLAIFã€è¿‡ç¨‹å¥–åŠ±ï¼‰
- Simulationï¼ˆå·¥å…·/ç¨‹åºï¼‰ï¼šè¯­è¨€é©±åŠ¨å¤–éƒ¨å·¥å…·ä¸ç¨‹åºè¿è¡Œï¼ˆToolformerã€Function Callingã€PALã€ä»£ç /æ±‚è§£å™¨ï¼‰
- Societyï¼ˆç¤¾ä¼šåŒ–åä½œï¼‰ï¼šå¤šæ™ºèƒ½ä½“/è¾©è®º/è¯„å®¡ï¼ˆDebateã€LLM-as-a-Judgeã€Self-Refine/Reflexionï¼‰



### MCTS


![llava-cot](./assets/llava_cot.png)

Timeline of o1-style releases (horizontal)

|              | Sep 12 | Oct 09 | Nov 04 | Nov 15 | Nov 16 | Nov 20 | Nov 25 | Nov 28 |
|--------------|------------|------------|------------|------------|------------|------------|------------|------------|
| Release      | OpenAI o1  | O1-Journey | LLaMA-O1   | LLaVA-CoT  | K0-math    | DeepSeek-R1| InternThinker | QwQ      |
| Organization | OpenAI     | SJTU       | Shanghai AI Lab | PKU     | Moonshot AI | DeepSeek  | Shanghai AI Lab | Alibaba Group |



[A] Xu Guowei et al., LLaVA-CoT: Let Vision Language Models Reason Step-by-Step, in arXiv, 2024.


### R1-Style Reasoning Models Overview

Note: The following table compiles notable r1-style models and resources.
<!-- table begins -->

| Model | Foundational LLMs | Time | Institution | Task | Feature |
|-------|------------------|------|-------------|------|---------|
| Deepseek-R1-Zero [![GitHub stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1?style=social&label=GitHub&logo=github)](https://github.com/deepseek-ai/DeepSeek-R1) | Deepseek-V3-671B | Jan 22, 2025 | DeepSeek-AI | Generic | - |
| Open-R1 [![GitHub stars](https://img.shields.io/github/stars/huggingface/open-r1?style=social&label=GitHub&logo=github)](https://github.com/huggingface/open-r1) | Qwen2.5-1.5B-Instruct | Jan 24, 2025 | HuggingFace | Generic | - |
| Multimodal-Open-R1 [![GitHub stars](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal?style=social&label=GitHub&logo=github)](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal) | Qwen2-VL-2B/7B-Instruct | Jan 27, 2025 | LMMs-Lab | Generic | - |
| R1-V [![GitHub stars](https://img.shields.io/github/stars/Deep-Agent/R1-V?style=social&label=GitHub&logo=github)](https://github.com/Deep-Agent/R1-V) | Qwen2-VL-2B-Instruct | Feb 2, 2025 | Deep Agent | Math | - |
| VLM-R1 [![GitHub stars](https://img.shields.io/github/stars/om-ai-lab/VLM-R1?style=social&label=GitHub&logo=github)](https://github.com/om-ai-lab/VLM-R1) | Qwen2.5-VL-3B/7B | Feb 3, 2025 | Zhejiang University | Object Detection | - |
| MedVLM-R1 | Qwen2-VL-2B | Feb 26, 2025 | Technical University of Munich | Medical Image Analysis | - |
| R1-Omni [![GitHub stars](https://img.shields.io/github/stars/HumanMLLM/R1-Omni?style=social&label=GitHub&logo=github)](https://github.com/HumanMLLM/R1-Omni) | HumanOmni-0.5B | Mar 7, 2025 | Chinese Academy of Sciences | Generic | - |
| MM-Eureka-Zero | InternVL2.5-Pretrained-8B | Mar 7, 2025 | Shanghai AI Lab | Math | - |
| VisualThinker-R1-Zero [![GitHub stars](https://img.shields.io/github/stars/turningpoint-ai/VisualThinker-R1-Zero?style=social&label=GitHub&logo=github)](https://github.com/turningpoint-ai/VisualThinker-R1-Zero) | Qwen2-VL-2B | Mar 7, 2025 | University of California | Math | "Aha Moment" on a 2B Non-SFT Model |
| Seg-Zero [![GitHub stars](https://img.shields.io/github/stars/dvlab-research/Seg-Zero?style=social&label=GitHub&logo=github)](https://github.com/dvlab-research/Seg-Zero) | Qwen2.5-VL-3B + SAM2 | Mar 9, 2025 | CUHK | Segmentation | - |
| Vision-R1 [![GitHub stars](https://img.shields.io/github/stars/Osilly/Vision-R1?style=social&label=GitHub&logo=github)](https://github.com/Osilly/Vision-R1) | Qwen-2.5-VL-72B | Mar 9, 2025 | Zhejiang University | Math | - |
| MM-Eureka | InternVL2.5-Instruct-8B | Mar 10, 2025 | Shanghai AI Laboratory | Math | Leave-One-Out, RLOO |
| LMM-R1 | Qwen2.5-VL-Instruct-3B | Mar 10, 2025 | Southeast University | Math, ScienceQA, ChartQA | Game Planning, PPO |
| Curr-ReFT | Qwen2.5-VL-3B | Mar 10, 2025 | USTC | Detection/Classification/Math | - |
| AlphaDrive | Qwen2VL-2B | Mar 10, 2025 | HUST | Autonomous driving | - |
| DriveLMM-o1 [![GitHub stars](https://img.shields.io/github/stars/mbzuai-oryx/DriveLMM-o1?style=social&label=GitHub&logo=github)](https://github.com/mbzuai-oryx/DriveLMM-o1) | InternVL2.5-8B | Mar 13, 2025 | MBZUAI | Autonomous driving | - |
| R1-OneVision [![GitHub stars](https://img.shields.io/github/stars/Fancy-MLLM/R1-Onevision?style=social&label=GitHub&logo=github)](https://github.com/Fancy-MLLM/R1-Onevision) | Qwen2.5-VL-7B-Instruct | Mar 13, 2025 | Zhejiang University | Math/General/Science/Chart | Formal Description |
| R1-VL [![GitHub stars](https://img.shields.io/github/stars/jingyi0000/R1-VL?style=social&label=GitHub&logo=github)](https://github.com/jingyi0000/R1-VL) | Qwen2-VL-7B | Mar 17, 2025 | NYTU | Math | Step-wise Reward |
| OpenVLThinker [![GitHub stars](https://img.shields.io/github/stars/yihedeng9/OpenVLThinker?style=social&label=GitHub&logo=github)](https://github.com/yihedeng9/OpenVLThinker) | Qwen2.5-VL-7B-Instruct | Mar 21, 2025 | University of California | Math | - |
| Easy-R1 | Qwen2.5-VL | Mar 21, 2025 | Beihang University | Math | Efficient, Scalable |
| Safe RLHF-V | Qwen2-VL-7B | Mar 22, 2025 | Peking University | Multimodal Safety | - |
| Video-R1 [![GitHub stars](https://img.shields.io/github/stars/tulerfeng/Video-R1?style=social&label=GitHub&logo=github)](https://github.com/tulerfeng/Video-R1) | Qwen2.5-VL-7B | Mar 27, 2025 | CUHK | Video Reasoning | - |
| Open-R1-Video [![GitHub stars](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video?style=social&label=GitHub&logo=github)](https://github.com/Wang-Xiaodong1899/Open-R1-Video) | Qwen2-VL-7B | Mar 27, 2025 | CUHK | Video Understanding | - |
| Embodied-Reasoner [![GitHub stars](https://img.shields.io/github/stars/zwq2018/embodied_reasoner?style=social&label=GitHub&logo=github)](https://github.com/zwq2018/embodied_reasoner) | Qwen2-VL-7B | Mar 27, 2025 | Zhejiang University | Embodied Interactive | Observationâ€“Thoughtâ€“Action |
| UI-R1 | Qwen2.5-VL-3B | Mar 27, 2025 | vivo AI Lab | Action Prediction of GUI Agents | - |
| Q-Insight [![GitHub stars](https://img.shields.io/github/stars/bytedance/Q-Insight?style=social&label=GitHub&logo=github)](https://github.com/bytedance/Q-Insight) | Qwen-2.5-VL-7B | Mar 28, 2025 | Peking University | Image Quality Assessment | - |

Note: A small GitHub badge next to a model name links to its confirmed repository. If no badge is shown, the official repo is pending or unverified.

<!-- Legend removed as Modality column was dropped -->

<!-- table ends -->

### Reasoning as &lt;think&gt;

#### Open R1 Video

[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://github.com/Wang-Xiaodong1899/Open-R1-Video)
 [![GitHub stars](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video?style=social&label=GitHub&logo=github)](https://github.com/Wang-Xiaodong1899/Open-R1-Video)

![Open R1 Video](assets/image-20250620102641966.png)

---

#### Video-R1: Reinforcing Video Reasoning in MLLMs

[![arXiv](https://img.shields.io/badge/arXiv-2503.21776-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2503.21776)
[![Zhihu](https://img.shields.io/badge/Zhihu-Review-informational?logo=zhihu)](https://zhuanlan.zhihu.com/p/1889342435928282728)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://github.com/tulerfeng/Video-R1)
 [![GitHub stars](https://img.shields.io/github/stars/tulerfeng/Video-R1?style=social&label=GitHub&logo=github)](https://github.com/tulerfeng/Video-R1)

![Video-R1](assets/image-20250620102641966.png)

---

#### VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning

[![arXiv](https://img.shields.io/badge/arXiv-2504.06958-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2504.06958)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://github.com/OpenGVLab/VideoChat-R1)
 [![GitHub stars](https://img.shields.io/github/stars/OpenGVLab/VideoChat-R1?style=social&label=GitHub&logo=github)](https://github.com/OpenGVLab/VideoChat-R1)

![VideoChat-R1](assets/image-20250620144735572.png)

---

#### TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning

[![arXiv](https://img.shields.io/badge/arXiv-2504.09641-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2504.09641)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1)
 [![GitHub stars](https://img.shields.io/github/stars/ZhangXJ199/TinyLLaVA-Video-R1?style=social&label=GitHub&logo=github)](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1)

![TinyLLaVA-Video-R1](assets/image-20250620103826723.png)

---

 

## ğŸ”€ Thinking across Modalities (Multimodal-level)

ä¸€å¥è¯æ¦‚è§ˆï¼šè®©â€œæ€ç»´é“¾â€è·¨è¶Šæ–‡æœ¬ä¸è§†è§‰ï¼ˆå›¾åƒ/è§†é¢‘/å›¾è¡¨ç­‰ï¼‰ï¼Œé€šè¿‡GRPO/DPO/RLHFä¸è¿‡ç¨‹ç›‘ç£ï¼Œæå‡è·¨æ¨¡æ€ç†è§£ä¸æ¨ç†ã€‚

### r1-like å¤šæ¨¡æ€æ¨ç†

- Video-R1 â€” å¼ºåŒ–è§†é¢‘æ—¶ç©ºæ¨ç† [é¡¹ç›®](https://github.com/tulerfeng/Video-R1) Â· [arXiv](https://arxiv.org/pdf/2503.21776)
- VideoChat-R1 â€” æ—¶ç©ºæ„ŸçŸ¥å¼ºåŒ–å¾®è°ƒ [é¡¹ç›®](https://github.com/OpenGVLab/VideoChat-R1) Â· [arXiv](https://arxiv.org/pdf/2504.06958)
- TinyLLaVA-Video-R1 â€” å°å‚æ•°è§†é¢‘æ¨ç† [é¡¹ç›®](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1) Â· [arXiv](https://arxiv.org/pdf/2504.09641)
- R1-VL â€” è§†è§‰-è¯­è¨€é€æ­¥å¥–åŠ± [é¡¹ç›®](https://github.com/jingyi0000/R1-VL)
- Open-R1-Video â€” å¼€æºè§†é¢‘R1èŒƒå¼ [é¡¹ç›®](https://github.com/Wang-Xiaodong1899/Open-R1-Video)
- Multimodal-Open-R1 â€” é€šç”¨å¤šæ¨¡æ€R1 [é¡¹ç›®](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)

### è®­ç»ƒç­–ç•¥ä¸è¿‡ç¨‹è´¨é‡

- LLaVA-CoT â€” é€æ­¥æ€ç»´é“¾ç”¨äºå¤šæ¨¡æ€è¿‡ç¨‹ç›‘ç£ [arXiv](https://arxiv.org/abs/2410.21922)
- MM-Eureka / MM-Eureka-Zero â€” ç•™ä¸€æ³•ä¸RLOOå¼ºåŒ–æ ·å¼ [ç¤ºä¾‹](https://github.com/ShanghaiAILab/MM-Eureka)
- LMM-R1 / Easy-R1 â€” ç»æµé«˜æ•ˆçš„R1è®­ç»ƒ [LMM-R1](https://github.com/thu-SLT-Lab/LMM-R1) Â· [Easy-R1](https://github.com/thu-sigma-lab/Easy-R1)
- Safe RLHF-V â€” å¤šæ¨¡æ€å®‰å…¨å¯¹é½ [é¡¹ç›®](https://github.com/PKU-Alignment/Safe-RLHF-V)

> å°ç»“ï¼šå¤šæ¨¡æ€â€œæ€ç»´â€”æœç´¢â€”éªŒè¯â€é—­ç¯æ­£åœ¨æ ‡å‡†åŒ–ï¼Œæ ¸å¿ƒåœ¨äºè¿‡ç¨‹ç›‘ç£ï¼ˆPRMï¼‰ã€è¡Œä¸ºå¥–åŠ±ä¸ç¯å¢ƒæ ¡éªŒç›¸ç»“åˆã€‚


## ğŸ–¼ï¸ Thinking with Images (Perceptual-level)

### ç†è®ºéƒ¨åˆ†


### ä¸ºä»€ä¹ˆè¦ç”¨å·¥å…·


[![Wiki](https://img.shields.io/badge/Wiki-Visual%20Thinking-blue?logo=wikipedia)](https://en.wikipedia.org/wiki/Visual_thinking)
[![OpenAI](https://img.shields.io/badge/OpenAI-Thinking%20with%20Images-9cf?logo=openai)](https://openai.com/index/thinking-with-images/)

[![Guide](https://img.shields.io/badge/Guide-Embodied--AI--Guide-brightgreen?logo=github)](https://github.com/TianxingChen/Embodied-AI-Guide)
[![Qwen-Agent](https://img.shields.io/badge/Industry-Qwen--Agent-blueviolet?logo=github)](https://github.com/QwenLM/Qwen-Agent/tree/main)

### Collections


[![PAPO](https://img.shields.io/badge/GRPO%20Improvement-PAPO-orange?logo=github)](https://github.com/MikeWangWZHL/PAPO)

### To Sort

[![HyperCLIP](https://img.shields.io/badge/To--Sort-HyperCLIP-lightgrey?logo=github)](https://github.com/SJTU-DeepVisionLab/HyperCLIP)
[![TreeVGR](https://img.shields.io/badge/To--Sort-TreeVGR-lightgrey?logo=github)](https://github.com/Haochen-Wang409/TreeVGR)
[![HyperVD](https://img.shields.io/badge/To--Sort-HyperVD-lightgrey?logo=github)](https://github.com/xiaogangpeng/HyperVD)

[![PyVision arXiv](https://img.shields.io/badge/PyVision-arXiv-ff69b4?logo=arxiv)](https://arxiv.org/pdf/2507.07998) [![PyVision Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://agent-x.space/pyvision/)

[![Data-Juicer arXiv](https://img.shields.io/badge/Data--Juicer-arXiv-ff69b4?logo=arxiv)](https://arxiv.org/pdf/2407.11784) [![Data-Juicer Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://modelscope.github.io/data-juicer/en/main/docs/awesome_llm_data.html)

### Curiosity-driven Learning

Humans monitor learning progress in curiosity-driven exploration (NC 2021) [[paper](https://www.nature.com/articles/s41467-021-26196-w)]

Curiosity-driven Exploration by Self-supervised Prediction (PMLR 2017) [[paper](https://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf)]

Computational mechanisms of curiosity and goal-directed exploration (Neuroscience 2019) [[paper](https://elifesciences.org/articles/41703)]

### Foundation Models & Theory

d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning [![arXiv](https://img.shields.io/badge/arXiv-2504.12216-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2504.12216)

Hyperbolic Safety-Aware Vision-Language Models (CVPR 2025) [![GitHub stars](https://img.shields.io/github/stars/aimagelab/HySAC?style=social&label=GitHub&logo=github)](https://github.com/aimagelab/HySAC) [![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://aimagelab.github.io/HySAC/)

![HySAC method](assets/hysac-method.png)

LSNet: See Large, Focus Small [![arXiv](https://img.shields.io/badge/arXiv-2503.23135-b31b1b?logo=arxiv)](https://arxiv.org/abs/2503.23135) [![GitHub stars](https://img.shields.io/github/stars/THU-MIG/lsnet?style=social&label=GitHub&logo=github)](https://github.com/THU-MIG/lsnet)

A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for accelerating Large VLMs (CVPR 2025) [![GitHub stars](https://img.shields.io/github/stars/NUS-HPC-AI-Lab/SGL?style=social&label=GitHub&logo=github)](https://github.com/NUS-HPC-AI-Lab/SGL)

**VLsI**: **V**erbalized **L**ayer**s**-to-**I**nteractions from Large to Small Vision Language Models [![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://byungkwanlee.github.io/VLsI-page/) [![arXiv](https://img.shields.io/badge/arXiv-2412.01822-b31b1b?logo=arxiv)](https://arxiv.org/abs/2412.01822)

Boltzmann Attention Sampling for Image Analysis with Small Objects (CVPR 2025) [![arXiv](https://img.shields.io/badge/arXiv-2503.02841-b31b1b?logo=arxiv)](https://arxiv.org/abs/2503.02841) [![GitHub stars](https://img.shields.io/github/stars/microsoft/BoltzFormer?style=social&label=GitHub&logo=github)](https://github.com/microsoft/BoltzFormer)

EntitySeg Toolbox: Towards open-world and high-quality image segmentation (ICCV 2023) [![GitHub stars](https://img.shields.io/github/stars/qqlu/Entity?style=social&label=GitHub&logo=github)](https://github.com/qqlu/Entity) [![Paper](https://img.shields.io/badge/Paper-ICCV2023-blue)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qi_High_Quality_Entity_Segmentation_ICCV_2023_paper.pdf)

### Image Manipulation

**Instruction-Guided Visual Masking** [[paper](https://arxiv.org/pdf/2405.19783)] [[code](https://github.com/2toinf/IVM)]

Plug-and-play module: mask irrelevant regions to enable better understanding by large models.

![Instruction-Guided Visual Masking example](assets/image-20250620101110725.png)

**COGCOM: A VISUAL LANGUAGE MODEL WITH CHAIN-OF-MANIPULATIONS REASONING** [[paper](https://arxiv.org/pdf/2402.04236)] [[code](https://github.com/THUDM/CogCoM)]

Chain of manipulations; intrinsic operations (e.g., locate, zoom) that produce intermediate outputs (e.g., bounding boxes, image patches).

![CogCoM chain-of-manipulations example](assets/image-20250620101001450.png)

Number it: Temporal Grounding Videos like Flipping Manga (CVPR 2025) [![arXiv](https://img.shields.io/badge/arXiv-2411.10332-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2411.10332)

![Flipping Manga temporal grounding example](assets/image-20250623093048591.png)

### Video Anomaly Understanding

(Content omitted here. See the original think-with-image.md for details and add as needed.)

 

## ğŸ–±ï¸ Thinking with Visual Operations (Interactive-level)

ä¸€å¥è¯æ¦‚è§ˆï¼šé€šè¿‡â€œå¯è§åŠ¨ä½œâ€è¿æ¥æ„ŸçŸ¥ä¸æ§åˆ¶ï¼ŒåŒ…æ‹¬ç•Œé¢æ“ä½œã€å¯è§†åŒ–å®šä½ã€å±å¹•ç†è§£ä¸å¯ç¼–æ’å·¥å…·é“¾ã€‚

### GUI ä»£ç†ä¸å±å¹•æ“ä½œ
- UI-R1 â€” å›¾å½¢ç•Œé¢æ™ºèƒ½ä½“åŠ¨ä½œé¢„æµ‹ [é¡¹ç›®](https://github.com/vivo-ai-lab/UI-R1)
- Qwen-Agent â€” å·¥å…·å¢å¼ºä¸GUIè‡ªåŠ¨åŒ–ç”Ÿæ€ [é¡¹ç›®](https://github.com/QwenLM/Qwen-Agent/tree/main)

### è§†è§‰æ“ä½œåŸè¯­ï¼ˆZoom/Locate/Segmentï¼‰
- CogCoMï¼ˆChain-of-Manipulationsï¼‰ [arXiv](https://arxiv.org/pdf/2402.04236) Â· [ä»£ç ](https://github.com/THUDM/CogCoM)
- Instruction-Guided Visual Masking [arXiv](https://arxiv.org/pdf/2405.19783) Â· [ä»£ç ](https://github.com/2toinf/IVM)
- EntitySeg Toolboxï¼ˆå¼€æ”¾ä¸–ç•Œåˆ†å‰²ï¼‰ [ä»£ç ](https://github.com/qqlu/Entity)

### å±å¹•/è§†é¢‘ä¸­çš„æ—¶ç©ºå®šä½
- Number it: Temporal Grounding Videos like Flipping Manga [arXiv](https://arxiv.org/pdf/2411.10332)

> å°ç»“ï¼šå°†â€œæ€ç»´é“¾â€ä¸â€œæ“ä½œé“¾â€ç»“åˆï¼Œèƒ½åœ¨äº¤äº’å¼ä»»åŠ¡ä¸­äº§ç”Ÿå¯æ£€éªŒçš„ä¸­é—´çŠ¶æ€ï¼Œä»è€Œåˆ©äºè¯„å®¡ä¸å¼ºåŒ–ã€‚


## ğŸ¤– Thinking in the Physical World (Embodied-level)

### Embodied Intelligence

Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation

#### Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks

[![arXiv](https://img.shields.io/badge/arXiv-2503.21696-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2503.21696v1)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://embodied-reasoner.github.io/)
[![GitHub stars](https://img.shields.io/github/stars/zwq2018/embodied_reasoner?style=social&label=GitHub&logo=github)](https://github.com/zwq2018/embodied_reasoner)

![Embodied-Reasoner](assets/image-20250620115046795.png)

---

#### Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning

[![arXiv](https://img.shields.io/badge/arXiv-2503.20752-b31b1b?logo=arxiv)](https://arxiv.org/abs/2503.20752)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://tanhuajie.github.io/ReasonRFT/)
[![GitHub stars](https://img.shields.io/github/stars/tanhuajie/Reason-RFT?style=social&label=GitHub&logo=github)](https://github.com/tanhuajie/Reason-RFT)

![Reason-RFT](assets/teaser.png)

---

#### Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation

[![CVPR 2025](https://img.shields.io/badge/CVPR2025-Paper-blueviolet?logo=opencv)](https://openaccess.thecvf.com/content/CVPR2025/papers/Yao_Think_Small_Act_Big_Primitive_Prompt_Learning_for_Lifelong_Robot_CVPR_2025_paper.pdf)

![Think Small, Act Big](assets/image-20250630111854316.png)

---

#### OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation

[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://shailab-ipec.github.io/openfly/)

![OpenFly](assets/toolchain.png)

---

#### SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via RL

[![arXiv](https://img.shields.io/badge/arXiv-2505.22596-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2505.22596)

![SAM-R1](assets/image-20250630112453520.png)

---

#### Visual-RFT: Visual Reinforcement Fine-Tuning

[![arXiv](https://img.shields.io/badge/arXiv-2503.01785-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2503.01785)
[![GitHub stars](https://img.shields.io/github/stars/Liuziyu77/Visual-RFT?style=social&label=GitHub&logo=github)](https://github.com/Liuziyu77/Visual-RFT)

![Visual-RFT](assets/image-20250630104729762.png)

---

#### Visual Planning: Let's Think Only with Images

[![arXiv](https://img.shields.io/badge/arXiv-2505.11409-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2505.11409)   [![GitHub stars](https://img.shields.io/github/stars/yix8/VisualPlanning?style=social&label=GitHub&logo=github)](https://github.com/yix8/VisualPlanning)

![Visual Planning](assets/image-20250630105040047.png)

---

#### AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving

[![arXiv](https://img.shields.io/badge/arXiv-2505.15298-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2505.15298)

![AgentThink](assets/image-20250725211337583.png)

---

## ğŸ› ï¸ Tutorials and Tooling

### å¼ºåŒ–å­¦ä¹ ç®—æ³•æ”¹è¿›

Agentic Reinforced Policy Optimization



å®ç°
swift agent-rl



TOOLLLM: Facilitating Large Language Models to Master 16000+ Real-World APIs <sup><kbd>ICLR 2024</kbd></sup> [![OpenReview](https://img.shields.io/badge/OpenReview-Link-green?logo=openreview)](https://openreview.net/forum?id=dHng2O0Jjr)

ReAct: Synergizing Reasoning and Acting in Language Models <sup><kbd>ICLR 2023</kbd></sup> [![arXiv](https://img.shields.io/badge/arXiv-2210.03629-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2210.03629)



##### è§‚ç‚¹æ–‡ç« 

ä¸ºä½•GRPOå¤§æ”¾å¼‚å½©DPOé”€å£°åŒ¿è¿¹ï¼Ÿ
[WeChat article](https://mp.weixin.qq.com/s/b4OkzqfRcpFhPzTocwJatw)


## ğŸ“– Related Collections

[![Awesome_Think_With_Images](https://img.shields.io/badge/Awesome-Think_With_Images-black?logo=github)](https://github.com/zhaochen0110/Awesome_Think_With_Images) â€” Visual-only reasoning with images (papers + code).

[![Awesome-Thinking-With-Images](https://img.shields.io/badge/Awesome-Thinking_With_Images-black?logo=github)](https://github.com/ligeng0197/Awesome-Thinking-With-Images) â€” Broad visual thinking and perception resources.

[open-thought/system-2-research](https://github.com/open-thought/system-2-research)

[Zanette-Labs/efficient-reasoning](https://github.com/Zanette-Labs/efficient-reasoning)