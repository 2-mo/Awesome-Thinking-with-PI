# Awesome LLM4VAD

A curated list of papers and resources on Large Language Models for Video Anomaly Detection (VAD).

## Contents

- [Overview](#overview)
- [Papers (2025)](#papers-2025)
- [Papers (2024)](#papers-2024)
- [Contributing](#contributing)
- [License and Credits](#license-and-credits)

---

## Overview

This list collects representative works that leverage LLMs or vision-language models for video anomaly detection, explanation, and understanding. Entries are grouped by year with links to paper and code, plus a preview figure when available.

---

## Papers (2025)

### VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models (CVPR 2025)

[![CVPR](https://img.shields.io/badge/CVPR-2025-1E90FF)](https://openaccess.thecvf.com/content/CVPR2025/papers/Ye_VERA_Explainable_Video_Anomaly_Detection_via_Verbalized_Learning_of_Vision-Language_CVPR_2025_paper.pdf)
[![Code](https://img.shields.io/github/stars/vera-framework/VERA?style=social&label=Code&logo=github)](https://github.com/vera-framework/VERA)

Highlight: Verbalized learning makes VLM-based VAD explainable with natural-language rationales and clearer decision traces.

![VERA preview](./assets/2025-cvpr-vera.png)

---

### Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity (CVPR 2025)

[![CVPR](https://img.shields.io/badge/CVPR-2025-1E90FF)](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.pdf)
[![Code](https://img.shields.io/github/stars/pipixin321/HolmesVAU?style=social&label=Code&logo=github)](https://github.com/pipixin321/HolmesVAU)

Highlight: Targets long-horizon anomaly understanding with fine-to-coarse granularity, improving temporal coverage and robustness.

![Holmes-VAU preview](./assets/2025-cvpr-holmes-vau.png)

---

## Papers (2024)

### VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection (AAAI 2024)

[![AAAI](https://img.shields.io/badge/AAAI-2024-1F77B4)](https://arxiv.org/abs/2308.11681)
[![arXiv](https://img.shields.io/badge/arXiv-2308.11681-b31b1b?logo=arxiv)](https://arxiv.org/abs/2308.11681)
[![Code](https://img.shields.io/github/stars/nwpu-zxr/VadCLIP?style=social&label=Code&logo=github)](https://github.com/nwpu-zxr/VadCLIP)

Highlight: Adapts CLIP-style visionâ€“language alignment to weakly supervised VAD, reducing annotation demands.

![VadCLIP preview](./assets/2024-aaai-vadclip.png)

---

### Harnessing Large Language Models for Training-free Video Anomaly Detection (CVPR 2024)

[![CVPR](https://img.shields.io/badge/CVPR-2024-1E90FF)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zanella_Harnessing_Large_Language_Models_for_Training-free_Video_Anomaly_Detection_CVPR_2024_paper.pdf)
[![Code](https://img.shields.io/github/stars/lucazanella/lavad?style=social&label=Code&logo=github)](https://github.com/lucazanella/lavad)

Highlight: Leverages LLM priors for training-free anomaly detection via promptable semantic knowledge.

![Training-free VAD preview](./assets/2024-cvpr-training-free-vad.png)

---

### Follow the Rules: Reasoning for Video Anomaly Detection with Large Language Models (ECCV 2024)

[![ECCV](https://img.shields.io/badge/ECCV-2024-0B84FE)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10568.pdf)
[![Code](https://img.shields.io/github/stars/Yuchen413/AnomalyRuler?style=social&label=Code&logo=github)](https://github.com/Yuchen413/AnomalyRuler)

Highlight: Injects rule-based reasoning with LLMs to guide anomaly decisions and improve interpretability.

![AnomalyRuler preview](./assets/2024-eccv-anomalyruler.png)

---

### Video Anomaly Detection and Explanation via Large Language Models (arXiv 2024)

[![arXiv](https://img.shields.io/badge/arXiv-2401.05702v1-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2401.05702v1)

Highlight: Couples VAD with LLM-generated explanations to provide interpretable, text-based rationales.

![LLM VAD + Explanation preview](./assets/2024-arxiv-vad-llm-explanation.png)

---

### HAWK: Learning to Understand Open-World Video Anomalies (NeurIPS 2024)

[![NeurIPS](https://img.shields.io/badge/NeurIPS-2024-2DB55D)](https://arxiv.org/pdf/2405.16886)
[![arXiv](https://img.shields.io/badge/arXiv-2405.16886-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2405.16886)
[![Code](https://img.shields.io/github/stars/jqtangust/hawk?style=social&label=Code&logo=github)](https://github.com/jqtangust/hawk)

Highlight: Pursues open-world anomaly understanding with scalable concept coverage and out-of-distribution robustness.

![HAWK preview](./assets/2024-neurips-hawk.png)

---

