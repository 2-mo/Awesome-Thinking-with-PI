<h1 align="center">Awesome Thinking with PI (<u>P</u>erception & <u>I</u>nteraction)</h1>

<p align="center">
  <b>A curated list of resources on visual reasoning, video understanding, embodied AI, robot action, and perception-driven interaction.</b>
</p>


<!-- Top badges -->
<p align="center">
  <a href="https://github.com/2-mo/Awesome-Thinking-with-PI/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-MIT-blueviolet" alt="License: MIT"></a>
  <a href="https://github.com/2-mo/Awesome-Thinking-with-PI/issues"><img src="https://img.shields.io/badge/Issues-Track-orange" alt="Issues"></a>
<a href="https://github.com/2-mo/Awesome-Thinking-with-PI/pulls"><img src="https://img.shields.io/badge/Pull%20Requests-Welcome-brightgreen" alt="Pull Requests"></a>
<a href="https://github.com/2-mo/Awesome-Thinking-with-PI/commits/main"><img src="https://img.shields.io/badge/Commits-Main-blue" alt="Commits: main"></a>
</p>

<!-- [![repo](https://img.shields.io/badge/GitHub-2--mo%2FAwesome--Thinking--with--PI-black?logo=github)](https://github.com/2-mo/Awesome-Thinking-with-PI) -->



## ğŸ“š Contents

- ğŸ¤” [1. Why We Need Thinking?](#1-why-we-need-thinking)
  - [1.1 ç†è®ºåŸºç¡€](#11-ç†è®ºåŸºç¡€) - äººç±»è§†è§‰å¯ç¤º / å¤šæ¨¡æ€æŒ‘æˆ˜ / è§†é¢‘å¼‚å¸¸æ£€æµ‹
  - [1.2 æ ¸å¿ƒç†å¿µä¸æ–¹æ³•](#12-æ ¸å¿ƒç†å¿µä¸æ–¹æ³•) - OpenAIæ€æƒ³ / è§‚ç‚¹æ–‡ç« 
  - [1.3 å¯ä¿¡è§†è§‰æ™ºèƒ½](#13-å¯ä¿¡è§†è§‰æ™ºèƒ½) - å‘å±•è¶‹åŠ¿
  - [1.4 åº”ç”¨æ¡ˆä¾‹ä¸å·¥ä½œ](#14-åº”ç”¨æ¡ˆä¾‹ä¸å·¥ä½œ) - ä»£è¡¨æ€§å·¥ä½œ / æœ€æ–°è§‚ç‚¹
  - [1.5 ç†è®ºç ”ç©¶](#15-ç†è®ºç ”ç©¶) - å­¦æœ¯è®ºæ–‡
- ğŸ’­ [2. Thinking with Language](#2-thinking-with-language)
  - [2.1 Language as Explicit Thought](#21-language-as-explicit-thought) - CoT / åŸºç¡€æ¨ç†
  - [2.2 MCTS](#22-mcts-monte-carlo-tree-search) - è’™ç‰¹å¡æ´›æ ‘æœç´¢
  - [2.3 R1-Style Models](#23-r1-style-models-overviewæˆªè‡³-3-æœˆ) - r1-like models
  - [2.4 Text-Space Multimodal Reasoning](#24-text-space-multimodal-reasoning) - æ–‡æœ¬ç©ºé—´å¤šæ¨¡æ€æ¨ç†
  - [2.5 Tool-Augmented Reasoning](#25-tool-augmented-reasoningå·¥å…·å¢å¼ºæ¨ç†) - å·¥å…·ä½¿ç”¨ä¸å¢å¼º
- ğŸ” [3. Thinking with Images](#3-thinking-with-images)
  - [3.1 Region-Focus](#31-region-focus-non-rlhf) - åŒºåŸŸèšç„¦
  - [3.2 Image Manipulation](#32-image-manipulation) - å›¾åƒæ“ä½œ
  - [3.3 åŸºäºå›¾åƒäº¤äº’çš„ç†è§£/æ¨ç†](#33-åŸºäºå›¾åƒäº¤äº’çš„ç†è§£æ¨ç†) - äº¤äº’å¼ç†è§£
  - [3.4 å›¾åƒç”Ÿæˆ](#34-å›¾åƒç”Ÿæˆ) - ç”Ÿæˆä¸æƒ³è±¡
  - [3.5 GUI ä»£ç†ä¸å±å¹•æ“ä½œ](#35-gui-ä»£ç†ä¸å±å¹•æ“ä½œ) - ç•Œé¢äº¤äº’
- ğŸ¤– [4. Thinking with Embodiment](#4-thinking-with-embodiment)
  - [4.1 VLA](#41-vla-is-all-you-need-) - è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹
  - [4.2 å…·èº«å¯¼èˆª](#42-å…·èº«å¯¼èˆªæ— äººæœº) - å¯¼èˆªä¸æ“ä½œ
- ğŸ› ï¸ [5. Tutorials and Tooling](#5-tutorials-and-tooling)
  - [5.1 å¼ºåŒ–å­¦ä¹ æ¡†æ¶](#51-å¼ºåŒ–å­¦ä¹ æ¡†æ¶) - RL æ¡†æ¶
  - [5.2 å¼ºåŒ–å­¦ä¹ ç®—æ³•](#52-å¼ºåŒ–å­¦ä¹ ç®—æ³•) - ç®—æ³•ä¸ä¼˜åŒ–
  - [5.3 æ“ä½œå®ç°](#53-æ“ä½œå®ç°) - å®ç°å·¥å…·
- ğŸ“– [6. Related Collections](#6-related-collections)
  - [6.1 ç»¼è¿°æ–‡ç« ](#61-ç»¼è¿°æ–‡ç« ) - ç›¸å…³ç»¼è¿°

---

## 1. ğŸ¤” Why We Need Thinking?

æ— è®ºåœ¨äººç±»è§†è§‰è¿˜æ˜¯å¤šæ¨¡æ€æ¨¡å‹é‡Œï¼Œæ„ŸçŸ¥ç»™å‡ºçš„è§‚æµ‹å¾€å¾€ä¸å®Œå…¨ã€å«å™ªä¸”å¤šè§£ï¼Œå¯é å†³ç­–å¿…é¡»ä¾èµ–è·¨æ—¶æ•´åˆä¸å‡è®¾æ£€éªŒâ€”â€”è¿™å°±æ˜¯â€œæ€è€ƒâ€ã€‚

[![Wiki](https://img.shields.io/badge/Wiki-Thinking%2C%20Fast%20and%20Slow-blue?logo=wikipedia)](https://en.wikipedia.org/wiki/Thinking,_Fast,_and_Slow)

### 1.1 ç†è®ºåŸºç¡€

#### äººç±»è§†è§‰çš„å¯ç¤º

- **è¾“å…¥ä¸å®Œæ•´**ï¼šç¬æ—¶æ„ŸçŸ¥é›¶ç¢ï¼ŒäºŒç»´åˆ°ä¸‰ç»´å­˜åœ¨å¤©ç„¶æ­§ä¹‰ï¼Œä»…é ç›´æ¥æ„ŸçŸ¥å®¹æ˜“è¢«é”™è§‰ä¸é®æŒ¡è¯¯å¯¼ã€‚
- **é¢„æµ‹â€”æ ¡æ­£å¾ªç¯**ï¼šè§†è§‰ä¾èµ–è‡ªä¸Šè€Œä¸‹ä¸è‡ªä¸‹è€Œä¸Šçš„äº’åŠ¨ï¼Œé€šè¿‡å‡è®¾ç”Ÿæˆã€è¯¯å·®ä¿®æ­£æ¥æŠµå¾¡æ­§ä¹‰ã€‚
- **è·¨æ—¶ä¸ä¸»åŠ¨æ§åˆ¶**ï¼šç¨³å¥è¡Œä¸ºä¾èµ–è·¨æ—¶å› æœè¿½è¸ªã€é€Ÿåº¦â€“å‡†ç¡®æ€§æƒè¡¡ï¼Œä»¥åŠç›®æ ‡é©±åŠ¨çš„æ³¨æ„ä¸èµ„æºåˆ†é…ã€‚

#### å¤šæ¨¡æ€æ¨¡å‹çš„æŒ‘æˆ˜

- **å¤šæ¨¡æ€è¾“å…¥åŒæ ·ä¸å…¨**ï¼šå›¾åƒå¯èƒ½é®æŒ¡ï¼Œè¯­éŸ³å«å™ªï¼Œæ–‡æœ¬æ­§ä¹‰ï¼Œæ¨¡æ€é—´è¿˜å¯èƒ½äº’ç›¸å†²çªã€‚
- **å•æ¬¡ååº”æ˜“åå·®**ï¼šåªä¾èµ–â€œå¿«æ„ŸçŸ¥â€å®¹æ˜“è¢«å±€éƒ¨æˆ–é”™è¯¯çº¿ç´¢ç‰µå¼•ã€‚
- **æ€è€ƒæ‰èƒ½ç¨³å¥**ï¼šè·¨æ¨¡æ€/è·¨æ—¶é—´æ•´åˆï¼Œå‡è®¾æ¯”è¾ƒä¸æ£€éªŒï¼Œé£é™©ä¸‹è‡ªé€‚åº”è°ƒæ§ï¼Œæ‰èƒ½å°†å˜ˆæ‚ä¸å…¨çš„æ„ŸçŸ¥è½¬åŒ–ä¸ºå¯é å†³ç­–ã€‚

#### è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„å¯ç¤º
> å‚è§ [[LLM4VAD Â· Video Anomaly Detection]](./llm4vad/README.md)

### 1.2 æ ¸å¿ƒç†å¿µä¸æ–¹æ³•

#### OpenAIçš„æ€è€ƒæ–¹æ³•

AIä¹‹æ‰€ä»¥ä¼šäº§ç”Ÿå¹»è§‰ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬è®­ç»ƒå®ƒçš„æ–¹å¼ï¼Œä»ä¸€å¼€å§‹ï¼Œå°±åœ¨ç³»ç»Ÿæ€§åœ°å¥–åŠ±è¿™ç§çè’™çš„è¡Œä¸ºã€‚
[![OpenAI](https://img.shields.io/badge/OpenAI-Why%20LMs%20Hallucinate-9cf?logo=openai)](https://openai.com/zh-Hans-CN/index/why-language-models-hallucinate/)

AIé‡Œæœ€å¤§çš„Bugï¼Œå´ä¹Ÿæ˜¯äººç±»æ–‡æ˜æœ€ä¼Ÿå¤§çš„èµ·ç‚¹ã€‚
[![WeChat](https://img.shields.io/badge/WeChat-Article-07C160?logo=wechat)](https://mp.weixin.qq.com/s/brNtm8QLR3V9LHGznu9E2A)

### 1.3 å¯ä¿¡è§†è§‰æ™ºèƒ½

å¯ä¿¡å¹¶éä»…æ¥è‡ªâ€œå¯è§£é‡Šæ€§â€ï¼Œè€Œæ˜¯æ¥è‡ªé•¿æœŸè®­ç»ƒä¸çœŸå®ä¸–ç•Œçš„ç¨³å®šè¡¨ç°ï¼ˆå‚è§ä¸€æ¬¡æ¼”è®²ä¸­çš„æ¯”å–»ï¼šæˆ‘ä»¬ä¿¡ä»»é™Œç”Ÿå¸æœºï¼Œå¤šå› å¯é ç»éªŒè€Œéå®Œå…¨å¯è§£é‡Šçš„å¤§è„‘æœºç†ï¼‰ã€‚

> Paraphrase from the [[talk]](https://www.youtube.com/watch?v=NA6EH8r-IT0): In response to a question about interpretability, Kaiming He asksâ€”why do you trust a taxi driver you don't know? Not because the brain is fully interpretable, but because extensive realâ€‘world training and testing make performance reliable; just like airplanes are trusted after millions of flights. Interpretability matters, yet reliability is ultimately earned through empirical evidence.



#### è¶‹åŠ¿ä¸å‘å±•æ–¹å‘
å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨é™æ€å›¾åƒåŸºç¡€ä»»åŠ¡ï¼ˆå¦‚ç®€å•ç‰©ä½“è¯†åˆ«ã€æ¸…æ™°OCRã€ç”»é¢æè¿°ï¼‰å·²è¶‹è¿‘é¥±å’Œï¼ŒBenchmarkä»·å€¼å‡å¼±ã€‚ä½†åŠ¨æ€è§†é¢‘äº¤äº’ã€é«˜åˆ†è¾¨ç‡å¤æ‚åœºæ™¯ï¼ˆå¦‚æ‰‹æœºæ‹æ‘„çš„å€¾æ–œå¿«é€’é¢å•ã€å¼ºå…‰å¹²æ‰°ä¸‹çš„å±€éƒ¨ä¿¡æ¯æ•æ‰ï¼‰åŠæ·±åº¦é€»è¾‘æ¨ç†èƒ½åŠ›ä»å­˜æ˜¾è‘—çŸ­æ¿ã€‚æœªæ¥æ ¸å¿ƒæ–¹å‘èšç„¦åŠ¨æ€å»ºæ¨¡å¼ºåŒ–ã€é•¿å°¾åœºæ™¯æ³›åŒ–æ€§æå‡ï¼Œå¹¶æ¨è¿›ç”µå•†ç­‰å‚ç›´é¢†åŸŸå®ç”¨åŒ–è½åœ°ï¼ˆå¦‚å•†å“å±æ€§è‡ªåŠ¨å®¡æ ¸ï¼‰ã€‚
åŠ¨æ€å¤šæ¨¡æ€åœ¨çœŸå®ç¯å¢ƒé€‚åº”æ€§ä¸Šçš„çªç ´å°¤ä¸ºå…³é”®ï¼Œå°¤å…¶åœ¨å¼±å…‰æˆ–éç†æƒ³æ„å›¾ä¸‹ã€‚

#### OpenAIçš„æ ¸å¿ƒæ€æƒ³

Similar to how a human may think for a long time before responding to a difficult question, o1 uses a chain of thought when attempting to solve a problem. Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses. It learns to recognize and correct its mistakes. It learns to break down tricky steps into simpler ones. It learns to try a different approach when the current one isnâ€™t working. This process dramatically improves the modelâ€™s ability to reason.
ä¸äººç±»åœ¨å›ç­”éš¾é¢˜ä¹‹å‰å¯èƒ½ä¼šæ€è€ƒå¾ˆé•¿æ—¶é—´ç±»ä¼¼ï¼Œo1 åœ¨å°è¯•è§£å†³é—®é¢˜æ—¶ä¹Ÿä¼šä½¿ç”¨æ€ç»´é“¾ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œo1 å¯ä»¥å­¦ä¼šç£¨ç»ƒè‡ªå·±çš„æ€ç»´é“¾ï¼Œå¹¶å®Œå–„è‡ªå·±ä½¿ç”¨çš„ç­–ç•¥ã€‚å®ƒå­¦ä¼šè¯†åˆ«å’Œçº æ­£é”™è¯¯ã€‚å®ƒå­¦ä¼šæŠŠæ£˜æ‰‹çš„æ­¥éª¤åˆ†è§£æˆæ›´ç®€å•çš„æ­¥éª¤ã€‚å®ƒå­¦ä¼šåœ¨å½“å‰æ–¹æ³•æ— æ•ˆæ—¶å°è¯•ä¸åŒçš„æ–¹æ³•ã€‚è¿™ä¸€è¿‡ç¨‹æå¤§åœ°æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚

[![OpenAI Learning](https://img.shields.io/badge/OpenAI-Learning%20to%20Reason-9cf?logo=openai)](https://openai.com/zh-Hant/index/learning-to-reason-with-llms/)
[![OpenAI Thinking](https://img.shields.io/badge/OpenAI-Thinking%20with%20Images-9cf?logo=openai)](https://openai.com/index/thinking-with-images/)
[![Wiki](https://img.shields.io/badge/Wiki-Visual%20Thinking-blue?logo=wikipedia)](https://en.wikipedia.org/wiki/Visual_thinking)


### 1.4 åº”ç”¨æ¡ˆä¾‹ä¸å·¥ä½œ

#### ä»£è¡¨æ€§å·¥ä½œ

[![OpenAI o1](https://img.shields.io/badge/OpenAI-ChatGPT--o1-9cf?logo=openai)](https://openai.com/o1/)
[![DeepSeek-R1](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1?style=social&label=DeepSeek-R1&logo=github)](https://github.com/deepseek-ai/DeepSeek-R1)

#### æœ€æ–°è§‚ç‚¹

ä¸ºä½•GRPOå¤§æ”¾å¼‚å½©DPOé”€å£°åŒ¿è¿¹ï¼Ÿ
[![WeChat](https://img.shields.io/badge/WeChat-GRPO%20vs%20DPO-07C160?logo=wechat)](https://mp.weixin.qq.com/s/b4OkzqfRcpFhPzTocwJatw)

### 1.5 ç†è®ºç ”ç©¶

ä»è¯­è¨€ç©ºé—´åˆ°åƒç´ ç©ºé—´çš„æ€è€ƒæœ‰æ•ˆæ€§ï¼š

**High-level visual representations in the human brain are aligned with large language models**
[![Nature Machine Intelligence](https://img.shields.io/badge/Nature%20Machine%20Intelligence-2025-darkgreen?logo=nature)](https://www.nature.com/articles/s42256-025-01072-0)

> ç ”ç©¶å‘ç°äººç±»å¤§è„‘é«˜å±‚è§†è§‰è¡¨å¾ä¸å¤§è¯­è¨€æ¨¡å‹å­˜åœ¨æ˜¾è‘—å¯¹é½ï¼Œä¸ºAIç³»ç»Ÿçš„è§†è§‰-è¯­è¨€ç†è§£æä¾›äº†ç¥ç»ç§‘å­¦åŸºç¡€ã€‚è¯¥ç ”ç©¶è¡¨æ˜LLMåµŒå…¥èƒ½å¤ŸæˆåŠŸè¡¨å¾å¤§è„‘ä»è‡ªç„¶åœºæ™¯ä¸­æå–çš„å¤æ‚è§†è§‰ä¿¡æ¯ï¼Œæ”¯æŒäº†å¤šæ¨¡æ€"æ€è€ƒ"çš„ç”Ÿç‰©å­¦åˆç†æ€§ã€‚

**A Theoretical Understanding of Self-Correction through In-context Alignment** (NeurIPS 2024)
Wang Yifei et al.

![Self-Correction In-Context Alignment](./assets/self-correction-incontext-alignment.png)

**ExpeL: LLM Agents Are Experiential Learners** (AAAI 2024)
Zhao Andrew et al.

---

ç†è§£äº†â€œæ€è€ƒâ€çš„å¿…è¦æ€§ä¹‹åï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢å¦‚ä½•è®©æ¨¡å‹å®ç°è¿™ä¸€è¿‡ç¨‹ã€‚åœ¨å½“å‰çš„æŠ€æœ¯èŒƒå¼ä¸­ï¼Œè¯­è¨€æ˜¯æœ€ç›´æ¥ã€æœ€æ ¸å¿ƒçš„æ€ç»´è½½ä½“ã€‚æœ¬ç« èŠ‚å°†èšç„¦äºæ¨¡å‹å¦‚ä½•åˆ©ç”¨è¯­è¨€æ„å»ºæ˜¾å¼çš„æ¨ç†é“¾æ¡ï¼Œä»ç®€å•çš„â€œæ€ç»´é“¾â€ï¼ˆChain-of-Thoughtï¼‰é€æ­¥æ¼”è¿›åˆ°æ›´å¤æ‚çš„æœç´¢ä¸å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚

## 2. ğŸ’­ Thinking with Language

> CoT -> GoT -> MCTS -> RL

Chain-of-Thought (åŸºç¡€) 
    â†“
Tree/Graph of Thoughts (ç»“æ„åŒ–)
    â†“  
MCTS (æœç´¢ä¼˜åŒ–)
    â†“
GRPO/RLHF (å¼ºåŒ–å­¦ä¹ )
    â†“
R1-Style Models (ç°ä»£å®ç°)

### 2.1 Language as Explicit Thought

**Letâ€™s Verify Step by Step** (process supervision/PRM, OpenAI): [![arXiv](https://img.shields.io/badge/arXiv-2305.20050-b31b1b?logo=arxiv)](https://arxiv.org/abs/2305.20050) -- è¿‡ç¨‹å¥–åŠ±

**Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2201.11903-b31b1b?logo=arxiv)](https://arxiv.org/abs/2201.11903)

**Tree of Thoughts: Deliberate Problem Solving with Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2305.10601-b31b1b?logo=arxiv)](https://arxiv.org/abs/2305.10601)

**Graph of Thoughts: Solving Elaborate Problems with Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2308.09687-b31b1b?logo=arxiv)](https://arxiv.org/abs/2308.09687)

**Automatic Chain of Thought Prompting in Large Language Models (ICLR 2023)** [![arXiv](https://img.shields.io/badge/arXiv-2210.03493-b31b1b?logo=arxiv)](https://arxiv.org/abs/2210.03493)

**Self-Consistency Improves Chain of Thought Reasoning in Language Models (ICLR 2023)** 
[![arXiv](https://img.shields.io/badge/arXiv-2203.11171-b31b1b?logo=arxiv)](https://arxiv.org/abs/2203.11171)
å¤šæ ·åŒ–æ€è·¯æŠ•ç¥¨

**Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena (NeurIPS 2023 Datasets and Benchmarks Track)** 
[![arXiv](https://img.shields.io/badge/arXiv-2306.05685-b31b1b?logo=arxiv)](https://arxiv.org/abs/2306.05685)

Wang Yifei et al., A Theoretical Understanding of Self-Correction through In-context Alignment, NeurIPS 2024.

### 2.2 MCTS (Monte Carlo Tree Search)

è¿™ä¸ªæ˜¯ä¸€ä¸ªè¿‡æ¸¡ï¼Œo1åˆšå‡ºçš„æ—¶å€™æ²¡æœ‰æŠ€æœ¯æŠ¥å‘Šï¼Œç¤¾åŒºçŒœæµ‹çš„å®ç°æ–¹å¼ï¼Œåœ¨deepseek-r1ä¹‹åï¼Œå¤§å®¶éƒ½æ˜¯grpoäº†ï½

ç¤¾åŒºæ—©æœŸå¸¸å°† o1 çš„â€œé•¿æ€è€ƒâ€ç†è§£ä¸º ToT/MCTS é£æ ¼æœç´¢ï¼›åœ¨ DeepSeekâ€‘R1 ä¹‹åï¼Œä¸»æµå®ç°æ›´å¤šç»“åˆ GRPO/RFT ç­‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥æå‡è¿‡ç¨‹è´¨é‡ä¸ç¨³å®šæ€§ã€‚


![llava-cot](./assets/llava-cot.png)

LLaVA-CoT â€” é€æ­¥æ€ç»´é“¾ç”¨äºå¤šæ¨¡æ€è¿‡ç¨‹ç›‘ç£ [![arXiv](https://img.shields.io/badge/arXiv-2410.21922-b31b1b?logo=arxiv)](https://arxiv.org/abs/2410.21922)



**Timeline of o1-style releases**

|              | Sep 12 | Oct 09 | Nov 04 | Nov 15 | Nov 16 | Nov 20 | Nov 25 | Nov 28 |
|--------------|------------|------------|------------|------------|------------|------------|------------|------------|
| Release      | OpenAI o1  | O1-Journey | LLaMA-O1   | LLaVA-CoT  | K0-math    | DeepSeek-R1| InternThinker | QwQ      |
| Organization | OpenAI     | SJTU       | Shanghai AI Lab | PKU     | Moonshot AI | DeepSeek  | Shanghai AI Lab | Alibaba Group |



[A] Xu Guowei et al., LLaVA-CoT: Let Vision Language Models Reason Step-by-Step, in arXiv, 2024.


### 2.3 R1-Style Models Overviewï¼ˆæˆªè‡³ 3 æœˆï¼‰

<details>
<summary>Click to expand: R1-Style Models Overview (as of March)</summary>

| Model | Foundational LLMs | Time | Institution | Task | Feature |
|-------|------------------|------|-------------|------|---------|
| Deepseek-R1-Zero [![GitHub stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1?style=social&label=GitHub&logo=github)](https://github.com/deepseek-ai/DeepSeek-R1) | Deepseek-V3-671B | Jan 22, 2025 | DeepSeek-AI | Generic | - |
| Open-R1 [![GitHub stars](https://img.shields.io/github/stars/huggingface/open-r1?style=social&label=GitHub&logo=github)](https://github.com/huggingface/open-r1) | Qwen2.5-1.5B-Instruct | Jan 24, 2025 | HuggingFace | Generic | - |
| Multimodal-Open-R1 [![GitHub stars](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal?style=social&label=GitHub&logo=github)](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal) | Qwen2-VL-2B/7B-Instruct | Jan 27, 2025 | LMMs-Lab | Generic | - |
| R1-V [![GitHub stars](https://img.shields.io/github/stars/Deep-Agent/R1-V?style=social&label=GitHub&logo=github)](https://github.com/Deep-Agent/R1-V) | Qwen2-VL-2B-Instruct | Feb 2, 2025 | Deep Agent | Math | - |
| VLM-R1 [![GitHub stars](https://img.shields.io/github/stars/om-ai-lab/VLM-R1?style=social&label=GitHub&logo=github)](https://github.com/om-ai-lab/VLM-R1) | Qwen2.5-VL-3B/7B | Feb 3, 2025 | Zhejiang University | Object Detection | - |
| MedVLM-R1 | Qwen2-VL-2B | Feb 26, 2025 | Technical University of Munich | Medical Image Analysis | - |
| R1-Omni [![GitHub stars](https://img.shields.io/github/stars/HumanMLLM/R1-Omni?style=social&label=GitHub&logo=github)](https://github.com/HumanMLLM/R1-Omni) | HumanOmni-0.5B | Mar 7, 2025 | Chinese Academy of Sciences | Generic | - |
| MM-Eureka-Zero | InternVL2.5-Pretrained-8B | Mar 7, 2025 | Shanghai AI Lab | Math | - |
| VisualThinker-R1-Zero [![GitHub stars](https://img.shields.io/github/stars/turningpoint-ai/VisualThinker-R1-Zero?style=social&label=GitHub&logo=github)](https://github.com/turningpoint-ai/VisualThinker-R1-Zero) | Qwen2-VL-2B | Mar 7, 2025 | University of California | Math | "Aha Moment" on a 2B Non-SFT Model |
| Seg-Zero [![GitHub stars](https://img.shields.io/github/stars/dvlab-research/Seg-Zero?style=social&label=GitHub&logo=github)](https://github.com/dvlab-research/Seg-Zero) | Qwen2.5-VL-3B + SAM2 | Mar 9, 2025 | CUHK | Segmentation | - |
| Vision-R1 [![GitHub stars](https://img.shields.io/github/stars/Osilly/Vision-R1?style=social&label=GitHub&logo=github)](https://github.com/Osilly/Vision-R1) | Qwen-2.5-VL-72B | Mar 9, 2025 | Zhejiang University | Math | - |
| MM-Eureka | InternVL2.5-Instruct-8B | Mar 10, 2025 | Shanghai AI Laboratory | Math | Leave-One-Out, RLOO |
| LMM-R1 [![GitHub stars](https://img.shields.io/github/stars/thu-SLT-Lab/LMM-R1?style=social&label=GitHub&logo=github)](https://github.com/thu-SLT-Lab/LMM-R1) | Qwen2.5-VL-Instruct-3B | Mar 10, 2025 | Southeast University | Math, ScienceQA, ChartQA | Game Planning, PPO |
| Curr-ReFT | Qwen2.5-VL-3B | Mar 10, 2025 | USTC | Detection/Classification/Math | - |
| AlphaDrive | Qwen2VL-2B | Mar 10, 2025 | HUST | Autonomous driving | - |
| DriveLMM-o1 [![GitHub stars](https://img.shields.io/github/stars/mbzuai-oryx/DriveLMM-o1?style=social&label=GitHub&logo=github)](https://github.com/mbzuai-oryx/DriveLMM-o1) | InternVL2.5-8B | Mar 13, 2025 | MBZUAI | Autonomous driving | - |
| R1-OneVision [![GitHub stars](https://img.shields.io/github/stars/Fancy-MLLM/R1-Onevision?style=social&label=GitHub&logo=github)](https://github.com/Fancy-MLLM/R1-Onevision) | Qwen2.5-VL-7B-Instruct | Mar 13, 2025 | Zhejiang University | Math/General/Science/Chart | Formal Description |
| R1-VL [![GitHub stars](https://img.shields.io/github/stars/jingyi0000/R1-VL?style=social&label=GitHub&logo=github)](https://github.com/jingyi0000/R1-VL) | Qwen2-VL-7B | Mar 17, 2025 | NYTU | Math | Step-wise Reward |
| OpenVLThinker [![GitHub stars](https://img.shields.io/github/stars/yihedeng9/OpenVLThinker?style=social&label=GitHub&logo=github)](https://github.com/yihedeng9/OpenVLThinker) | Qwen2.5-VL-7B-Instruct | Mar 21, 2025 | University of California | Math | - |
| Easy-R1 | Qwen2.5-VL | Mar 21, 2025 | Beihang University | Math | Efficient, Scalable |
| Safe RLHF-V | Qwen2-VL-7B | Mar 22, 2025 | Peking University | Multimodal Safety | - |
| Video-R1 [![GitHub stars](https://img.shields.io/github/stars/tulerfeng/Video-R1?style=social&label=GitHub&logo=github)](https://github.com/tulerfeng/Video-R1) | Qwen2.5-VL-7B | Mar 27, 2025 | CUHK | Video Reasoning | - |
| Open-R1-Video [![GitHub stars](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video?style=social&label=GitHub&logo=github)](https://github.com/Wang-Xiaodong1899/Open-R1-Video) | Qwen2-VL-7B | Mar 27, 2025 | CUHK | Video Understanding | - |
| Embodied-Reasoner [![GitHub stars](https://img.shields.io/github/stars/zwq2018/embodied_reasoner?style=social&label=GitHub&logo=github)](https://github.com/zwq2018/embodied_reasoner) | Qwen2-VL-7B | Mar 27, 2025 | Zhejiang University | Embodied Interactive | Observationâ€“Thoughtâ€“Action |
| UI-R1 | Qwen2.5-VL-3B | Mar 27, 2025 | vivo AI Lab | Action Prediction of GUI Agents | - |
| Q-Insight [![GitHub stars](https://img.shields.io/github/stars/bytedance/Q-Insight?style=social&label=GitHub&logo=github)](https://github.com/bytedance/Q-Insight) | Qwen-2.5-VL-7B | Mar 28, 2025 | Peking University | Image Quality Assessment | - |

Note: A small GitHub badge next to a model name links to its confirmed repository. If no badge is shown, the official repo is pending or unverified.

<!-- Legend removed as Modality column was dropped -->

<!-- table ends -->

</details>

### 2.4 Text-Space Multimodal Reasoning

å®šä¹‰ï¼šr1â€‘like çš„è·¨æ¨¡æ€è®¾å®šï¼Œæ¨ç†é“¾ï¼ˆCoTï¼‰ä¸»è¦åœ¨æ–‡æœ¬ç©ºé—´è¿›è¡Œï¼›å¤šæ¨¡æ€ä¿¡å·ï¼ˆå›¾åƒ/è§†é¢‘/éŸ³é¢‘/å›¾è¡¨ç­‰ï¼‰ä½œä¸ºæ¡ä»¶ã€å¥–åŠ±æˆ–è¯„ä¼°ä¿¡å·å‚ä¸è®­ç»ƒä¸ç”Ÿæˆï¼Œè¾“å‡ºå¯ä¸ºå¤šæ¨¡æ€ï¼Œä½†ä¸­é—´ä¸äº§ç”Ÿè§†è§‰ç±»ä¸­é—´è¡¨å¾ã€‚

å…³æ³¨ï¼šåœ¨æ£€æµ‹/åˆ†å‰²/è§†é¢‘/åŒ»å­¦ç­‰ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨ GRPO/DPO/RLHF ä¸è¿‡ç¨‹ç›‘ç£æ”¹è¿›æ–‡æœ¬ç©ºé—´æ¨ç†çš„ç¨³å¥æ€§ä¸å¯¹é½æ•ˆæœã€‚

#### è¯„ä¼°åŸºå‡†/åŸºäºè¯„ä»·çš„

MM-Eureka / MM-Eureka-Zero â€” ç•™ä¸€æ³•ä¸RLOOå¼ºåŒ–æ ·å¼ [ç¤ºä¾‹](https://github.com/ShanghaiAILab/MM-Eureka)



**Vision-SR1: Self-Rewarded Vision-Language Model via Reasoning Decomposition**
è…¾è®¯AI Labç­‰æå‡ºè‡ªæˆ‘å¥–åŠ±æ–¹æ³•ï¼Œåˆ†é˜¶æ®µæå‡VLMæ¨ç†èƒ½åŠ›ï¼Œä»£ç å·²å¼€æºã€‚



**LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model**
**arXiv**ï¼š[arXiv:2509.00676](https://arxiv.org/abs/2509.00676)
**ä¸»è¦å†…å®¹**ï¼šåˆ†æç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹åœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæå‡ºLLaVA-Critic-R1ã€‚



#### ç›®æ ‡æ£€æµ‹/åˆ†å‰²

https://github.com/om-ai-lab/VLM-R1


**Visual-RFT: Visual Reinforcement Fine-Tuning (ICCV 2025)**

[![arXiv](https://img.shields.io/badge/arXiv-2503.01785-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2503.01785)
[![GitHub stars](https://img.shields.io/github/stars/Liuziyu77/Visual-RFT?style=social&label=GitHub&logo=github)](https://github.com/Liuziyu77/Visual-RFT)

![Visual-RFT](assets/visual-rft-architecture.png)


**MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models via RL (arXiv 2025)**

[![arXiv](https://img.shields.io/badge/arXiv-2502.19634-b31b1b?logo=arxiv)](https://arxiv.org/abs/2502.19634)

Highlight: é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±æœºåˆ¶æå‡åŒ»å­¦å¤šæ¨¡æ€æ¨ç†è¦†ç›–åº¦ä¸å¯é æ€§ã€‚

Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models (arXiv 2025)

[![arXiv](https://img.shields.io/badge/arXiv-2503.13939-b31b1b?logo=arxiv)](https://arxiv.org/abs/2503.13939) [![Code](https://img.shields.io/github/stars/Yuxiang-Lai117/Med-R1?style=social&label=Code&logo=github)](https://github.com/Yuxiang-Lai117/Med-R1)

Highlight: å…³æ³¨è·¨ä»»åŠ¡æ³›åŒ–çš„åŒ»å­¦æ¨ç† RL å¯¹é½èŒƒå¼ï¼Œæ”¹è¿›é²æ£’æ€§ä¸æ•°æ®æ•ˆç”¨ã€‚

<img src="https://github.com/Yuxiang-Lai117/Med-R1/raw/main/Images/fig_data_distribution.png" alt="Med-R1 dataset distribution" width="55%" />


SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via RL

[![arXiv](https://img.shields.io/badge/arXiv-2505.22596-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2505.22596)

![SAM-R1](assets/sam-r1-framework.png)

---

#### è§†é¢‘ç†è§£ 

Video-R1: Reinforcing Video Reasoning in MLLMs
å¼ºåŒ–è§†é¢‘æ—¶ç©ºæ¨ç† 
[![arXiv](https://img.shields.io/badge/arXiv-2503.21776-b31?logo=arxiv)](https://arxiv.org/pdf/2503.21776)
[![Zhihu](https://img.shields.io/badge/Zhihu-Review-informational?logo=zhihu)](https://zhuanlan.zhihu.com/p/1889342435928282728)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://github.com/tulerfeng/Video-R1)
[![GitHub stars](https://img.shields.io/github/stars/tulerfeng/Video-R1?style=social&label=GitHub&logo=github)](https://github.com/tulerfeng/Video-R1)

![Video-R1](assets/video-r1-overview.png)

**VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning**
[![arXiv](https://img.shields.io/badge/arXiv-2504.06958-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2504.06958)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://github.com/OpenGVLab/VideoChat-R1)
[![GitHub stars](https://img.shields.io/github/stars/OpenGVLab/VideoChat-R1?style=social&label=GitHub&logo=github)](https://github.com/OpenGVLab/VideoChat-R1)

> æ—¶ç©ºæ„ŸçŸ¥å¼ºåŒ–å¾®è°ƒ

![VideoChat-R1](assets/videochat-r1-overview.png)

TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning
å°å‚æ•°è§†é¢‘æ¨ç†
[![arXiv](https://img.shields.io/badge/arXiv-2504.09641-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2504.09641)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1)
[![GitHub stars](https://img.shields.io/github/stars/ZhangXJ199/TinyLLaVA-Video-R1?style=social&label=GitHub&logo=github)](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1)

![TinyLLaVA-Video-R1](assets/tinyllava-video-r1-overview.png)


#### Extended Vision Modalities
å…¶ä»–æ¨¡æ€ï¼ˆäº‹ä»¶ç›¸æœº,3D,çº¢å¤–ï¼‰




#### å¯ä¿¡å®‰å…¨

- Safe RLHF-V â€” å¤šæ¨¡æ€å®‰å…¨å¯¹é½ [é¡¹ç›®](https://github.com/PKU-Alignment/Safe-RLHF-V)

> å°ç»“ï¼šå¤šæ¨¡æ€â€œæ€ç»´â€”æœç´¢â€”éªŒè¯â€é—­ç¯æ­£åœ¨æ ‡å‡†åŒ–ï¼Œæ ¸å¿ƒåœ¨äºè¿‡ç¨‹ç›‘ç£ï¼ˆPRMï¼‰ã€è¡Œä¸ºå¥–åŠ±ä¸ç¯å¢ƒæ ¡éªŒç›¸ç»“åˆã€‚


### 2.5 Tool-Augmented Reasoningï¼ˆå·¥å…·å¢å¼ºæ¨ç†ï¼‰

**æ€ç»´+è¡ŒåŠ¨çš„äº¤æ›¿ï¼ˆæ£€ç´¢/å·¥å…·ä½¿ç”¨ï¼‰(2023)**

**ReAct: Synergizing Reasoning and Acting in Language Models** <sup><kbd>ICLR 2023</kbd></sup> [![arXiv](https://img.shields.io/badge/arXiv-2210.03629-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2210.03629)
> **Core Idea**: Proposes a paradigm that interleaves `thought` (reasoning) and `action` (tool use) to solve complex tasks.

**Program of Thoughts Prompting: Disentangling Computation from Reasoning** (TMLR 2023) [![arXiv](https://img.shields.io/badge/arXiv-2211.12588-b31b1b?logo=arxiv)](https://arxiv.org/abs/2211.12588)
> **Core Idea**: Offloads calculation steps to an external interpreter (e.g., Python), allowing the LLM to focus on reasoning.

**Toolformer: Language Models Can Teach Themselves to Use Tools** (arXiv 2023) [![arXiv](https://img.shields.io/badge/arXiv-2302.04761-b31b1b?logo=arxiv)](https://arxiv.org/abs/2302.04761)
> **Core Idea**: A method for teaching LLMs to decide when and how to use external tools through simple API calls, and to incorporate the results.

**TOOLLLM: Facilitating Large Language Models to Master 16000+ Real-World APIs** <sup><kbd>ICLR 2024</kbd></sup> [![OpenReview](https://img.shields.io/badge/OpenReview-Link-green?logo=openreview)](https://openreview.net/forum?id=dHng2O0Jjr)
> **Core Idea**: A framework and dataset for training and evaluating LLMs on their ability to use a vast number of real-world APIs.

**DSPy: A framework for algorithmically optimizing LM prompts and weights** [![GitHub stars](https://img.shields.io/github/stars/stanfordnlp/dspy?style=social&label=GitHub&logo=github)](https://github.com/stanfordnlp/dspy)
> **Core Idea**: A programmatic approach (`compiling` instead of `prompting`) that separates logic from prompts to build more reliable systems.

**AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?** [![Project](https://img.shields.io/badge/Project-Website-blue?logo=safari)](https://bingreeky.github.io/atracer/)
> **Team**: Shuicheng Yan's team (é¢œæ°´æˆè€å¸ˆå›¢é˜Ÿ)
> **Core Idea**: An automated framework for attributing failures in multi-agent systems, improving debugging and diagnosis efficiency.


## 3. ğŸ–¼ï¸ Thinking with Images 

ä»çº¯æ–‡æœ¬æ¨ç†åˆ°ä¸æ„ŸçŸ¥æ·±åº¦èåˆï¼Œæœ¬ç« å°†æ¢è®¨ä¸€ç§æ›´é«˜çº§çš„èŒƒå¼ï¼š**å›¾æ–‡äº¤é”™çš„å¤šæ¨¡æ€æ¨ç†** (Interleaved Vision-Language Reasoning)ã€‚

> è¿™ç§èŒƒå¼ä¸ r1-like æ¨¡å‹æœ‰ç€æœ¬è´¨åŒºåˆ«ã€‚r1-like æ¨¡å‹é€šå¸¸å°†å›¾åƒä½œä¸ºä¸€æ¬¡æ€§çš„åˆå§‹è¾“å…¥ï¼Œå…¶åçš„æ¨ç†é“¾å®Œå…¨åœ¨æ–‡æœ¬ç©ºé—´å±•å¼€ã€‚è€Œåœ¨è¿™é‡Œï¼Œ**è§†è§‰è¡¨å¾æœ¬èº«å°±æ˜¯æ¨ç†è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†**ã€‚æ¨¡å‹ä¼šä¸è§†è§‰ä¿¡æ¯è¿›è¡Œå¤šè½®äº¤äº’ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨ç”Ÿæˆå¹¶åˆ©ç”¨æ–°çš„è§†è§‰äº§ç‰©ï¼ˆå¦‚è£å‰ªã€æ ‡æ³¨ã€è‰å›¾ï¼‰ï¼Œå®ç°â€œè¾¹çœ‹è¾¹æƒ³ã€è¾¹ç”»è¾¹æƒ³â€çš„æ·±åº¦èåˆã€‚è¿™ä¸â€œThinking across Modalitiesâ€ä¸­æ¨ç†é“¾ä¸»è¦å±€é™äºæ–‡æœ¬ç©ºé—´å½¢æˆé²œæ˜å¯¹æ¯”ã€‚



### 3.1 Region-Focus (Non-RLHF)

LSNet: See Large, Focus Small [![arXiv](https://img.shields.io/badge/arXiv-2503.23135-b31b1b?logo=arxiv)](https://arxiv.org/abs/2503.23135) [![GitHub stars](https://img.shields.io/github/stars/THU-MIG/lsnet?style=social&label=GitHub&logo=github)](https://github.com/THU-MIG/lsnet)

A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for accelerating Large VLMs (CVPR 2025) [![GitHub stars](https://img.shields.io/github/stars/NUS-HPC-AI-Lab/SGL?style=social&label=GitHub&logo=github)](https://github.com/NUS-HPC-AI-Lab/SGL)

**VLsI**: **V**erbalized **L**ayer**s**-to-**I**nteractions from Large to Small Vision Language Models [![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://byungkwanlee.github.io/VLsI-page/) [![arXiv](https://img.shields.io/badge/arXiv-2412.01822-b31b1b?logo=arxiv)](https://arxiv.org/abs/2412.01822)

Boltzmann Attention Sampling for Image Analysis with Small Objects (CVPR 2025) [![arXiv](https://img.shields.io/badge/arXiv-2503.02841-b31b1b?logo=arxiv)](https://arxiv.org/abs/2503.02841) [![GitHub stars](https://img.shields.io/github/stars/microsoft/BoltzFormer?style=social&label=GitHub&logo=github)](https://github.com/microsoft/BoltzFormer)



### 3.2 Image Manipulation

**Number it: Temporal Grounding Videos like Flipping Manga**
[![arXiv](https://img.shields.io/badge/arXiv-2411.10332-b31b1b?logo=arxiv)](https://arxiv.org/abs/2411.10332)


**Instruction-Guided Visual Masking** [[paper](https://arxiv.org/pdf/2405.19783)] [[code](https://github.com/2toinf/IVM)]

Plug-and-play module: mask irrelevant regions to enable better understanding by large models.

![Instruction-Guided Visual Masking example](assets/instruction-masking-example.png)

**COGCOM: A VISUAL LANGUAGE MODEL WITH CHAIN-OF-MANIPULATIONS REASONING** [[paper](https://arxiv.org/pdf/2402.04236)] [[code](https://github.com/THUDM/CogCoM)]

Chain of manipulations; intrinsic operations (e.g., locate, zoom) that produce intermediate outputs (e.g., bounding boxes, image patches).

![CogCoM chain-of-manipulations example](assets/cogcom-chain-example.png)

Number it: Temporal Grounding Videos like Flipping Manga (CVPR 2025) [![arXiv](https://img.shields.io/badge/arXiv-2411.10332-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2411.10332)

![Flipping Manga temporal grounding example](assets/number-it-example.png)





### 3.3 åŸºäºå›¾åƒäº¤äº’çš„ç†è§£/æ¨ç†

**Simple o3: Towards Interleaved Vision-Language Reasoning**
[![arXiv](https://img.shields.io/badge/arXiv-2508.12109-b31b1b?logo=arxiv)](https://arxiv.org/abs/2508.12109)

> Interleaved Reasoning, Tool-Use, Visual Planning
> é¢å‘â€œå›¾æ–‡äº¤é”™æ¨ç†â€çš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œç»“åˆå¯æ‰§è¡Œè§†è§‰æ“ä½œï¼ˆè£å‰ª/æ”¾å¤§/å¤ç”¨å›¾åƒï¼‰ä¸è¯­è¨€æ¨ç†ï¼Œå¼ºåŒ–é•¿é“¾å¤šæ¨¡æ€æ¨ç†ä¸ç»†ç²’åº¦ç†è§£ã€‚


**Mini-o3: Scaling Up Reasoning Patterns and Interaction for Visual Search**

[![arXiv](https://img.shields.io/badge/arXiv-2509.07969-b31b1b?logo=arxiv)](https://arxiv.org/abs/2509.07969)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://mini-o3.github.io/)
[![GitHub stars](https://img.shields.io/github/stars/Mini-03/Mini-o3?style=social&label=GitHub&logo=github)](https://github.com/Mini-03/Mini-o3)
[![Zhihu](https://img.shields.io/badge/Zhihu-Review-informational?logo=zhihu)](https://zhuanlan.zhihu.com/p/1949143503058760011)

> å­—èŠ‚è·³åŠ¨ç­‰æå‡ºMini-o3ç³»ç»Ÿï¼Œé€šè¿‡å¤šæ­¥äº¤äº’å’Œæ·±åº¦æ¨ç†æå‡è§†è§‰æœç´¢ä»»åŠ¡è¡¨ç°ã€‚
> 
> ![alt text](./assets/mini-o3.png)


**LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA**
Ant Group & Nanyang Technological University
https://arxiv.org/abs/2509.10026
https://github.com/HJNVR/LaV-CoT

![alt text](./assets/lav-cot.png)


**Introducing Visual Perception Token into Multimodal Large Language Model**
[![arXiv](https://img.shields.io/badge/arXiv-2502.17425-b31b1b?logo=arxiv)](https://arxiv.org/abs/2502.17425)
[![GitHub stars](https://img.shields.io/github/stars/yu-rp/VisualPerceptionToken?style=social&label=GitHub&logo=github)](https://github.com/yu-rp/VisualPerceptionToken)

> æå‡ºè§†è§‰æ„ŸçŸ¥ä»¤ç‰Œæ¦‚å¿µï¼Œèµ‹äºˆMLLMè‡ªä¸»æ§åˆ¶è§†è§‰æ„ŸçŸ¥è¿‡ç¨‹çš„èƒ½åŠ›ã€‚åŒ…æ‹¬åŒºåŸŸé€‰æ‹©ä»¤ç‰Œå’Œè§†è§‰é‡ç¼–ç ä»¤ç‰Œä¸¤ç§ç±»å‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»è¯†åˆ«éœ€è¦è¿›ä¸€æ­¥æ„ŸçŸ¥çš„ç‰¹å®šåŒºåŸŸæˆ–ä½¿ç”¨éšè—çŠ¶æ€ä½œä¸ºæ§åˆ¶ä¿¡å·ã€‚
> 
> ![alt text](./assets/VisualPerceptionToken.png)
> è§†è§‰é‡ç¼–ç ï¼ˆVision Re-Encodingï¼‰è¿›ä¸€æ­¥è°ƒç”¨ DINO ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¯­è¨€æ¨¡å‹éšè—çŠ¶æ€æ§åˆ¶ç‰¹å¾é€‰æ‹©ï¼Œå®ç°â€œç²—çœ‹â€”å†ç»†çœ‹â€çš„ä¸¤æ­¥æ„ŸçŸ¥ï¼Œæ›´æ¥è¿‘äººç±»è§‚å¯Ÿè¿‡ç¨‹(ä¸“æ³¨åœ°çœ‹)ã€‚



**Thyme: Think Beyond Images**
> å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹Thymeï¼Œæ”¯æŒä¸»åŠ¨è°ƒç”¨å·¥å…·è¿›è¡Œå¤æ‚å›¾åƒå¤„ç†å’Œæ•°å­¦è®¡ç®—ï¼Œå…·å¤‡é«˜åº¦è‡ªä¸»æ€§ã€‚


**OCR Comparison: Expert Models vs. Vision-Language Models**
> ä»‹ç»é€šä¹‰å›¢é˜Ÿæå‡ºçš„ç‚¹é‡‘OCR-R1ï¼ŒåŠå…¶ä¸ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ–‡æ¡£è§£æä»»åŠ¡ä¸­çš„å¯¹æ¯”è¡¨ç°ã€‚


**Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation**
[![arXiv](https://img.shields.io/badge/arXiv-2508.12680-b31b1b?logo=arxiv)](https://arxiv.org/abs/2508.12680)

> UCSDç­‰æå‡ºå¤šé¢†åŸŸæ•°æ®æ•´ç†å’Œå¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒç®¡é“ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹¿æ³›æ¨ç†èƒ½åŠ›ã€‚


**PyVision: Enabling Fast, Interactive Visual Programming with AI**
[![PyVision arXiv](https://img.shields.io/badge/PyVision-arXiv-ff69b4?logo=arxiv)](https://arxiv.org/pdf/2507.07998) [![PyVision Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://agent-x.space/pyvision/)

> <img src="./assets/pyvision-overview.png" alt="PyVision overview" width="60%" />


**DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning**
[![arXiv](https://img.shields.io/badge/arXiv-2505.14362-b31b1b?logo=arxiv)](https://arxiv.org/abs/2505.14362)

> ![deepeyes-overview](./assets/deepeyes-overview.png)



**VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection**

![VisTA overview](./assets/vista-overview.png)


**REVPT: Reinforced Visual Perception with Tools**
[![GitHub stars](https://img.shields.io/github/stars/Is-kelvin/REVPT?style=social&label=GitHub&logo=github)](https://github.com/Is-kelvin/REVPT) [![arXiv](https://img.shields.io/badge/arXiv-2509.01656-b31b1b?logo=arxiv)](https://arxiv.org/abs/2509.01656)

æå‡ºREVPTæ–¹æ³•ï¼Œé€šè¿‡RLè®­ç»ƒå¤šæ¨¡æ€å¤§æ¨¡å‹ä½¿ç”¨è§†è§‰å·¥å…·ï¼Œæå‡æ„ŸçŸ¥-æ¨ç†èƒ½åŠ›ã€‚


**VLM-R3: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought**
[![arXiv](https://img.shields.io/badge/arXiv-2505.16192-b31b1b?logo=arxiv)](https://arxiv.org/abs/2505.16192)

> æå‡ºVLM-R3æ¡†æ¶ï¼Œå¼ºåŒ–MLLMåœ¨è§†è§‰åŒºåŸŸè¯†åˆ«ä¸æ¨ç†èƒ½åŠ›ï¼Œæå‡å¤šæ¨¡æ€é“¾å¼æ¨ç†è¡¨ç°ã€‚


**Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning**

> æå‡ºEgo-R1æ™ºèƒ½ä½“ï¼Œé‡‡ç”¨Chain-of-Tool-Thoughtæ–¹æ³•ï¼Œæ”¯æŒè¶…é•¿è‡ªä¸­å¿ƒè§†é¢‘æ¨ç†ã€‚


**M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation**
[![arXiv](https://img.shields.io/badge/arXiv-2508.06328-b31b1b?logo=arxiv)](https://arxiv.org/abs/2508.06328)

> åŒ—å¤§ç­‰æå‡ºçš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨RLä¼˜åŒ–å¤šæ¨¡æ€è¾“å…¥è¾“å‡ºï¼Œæå‡æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡ã€‚


**VLM-FO1: A New Object Detection Method for Vision-Language Models**
> ä»‹ç»VLM-FO1æ¨¡å‹ï¼Œé€šè¿‡â€œç”»æ¡†â€æ–¹å¼æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ£€æµ‹ç²¾åº¦ã€‚



#### è§†é¢‘ç†è§£

**VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning**
[![arXiv](https://img.shields.io/badge/arXiv-2506.06097-b31b1b?logo=arxiv)](https://arxiv.org/abs/2506.06097)

ç‹åˆ©æ°‘è€å¸ˆå›¢é˜Ÿï¼Œä¹”å®‡è€å¸ˆ

![VideoChat-A1](./assets/videochat-a1-overview.png)

**RynnEC: Bringing MLLMs into Embodied World**
[![GitHub stars](https://img.shields.io/github/stars/alibaba-damo-academy/RynnEC?style=social&label=GitHub&logo=github)](https://github.com/alibaba-damo-academy/RynnEC)

é˜¿é‡Œè¾¾æ‘©é™¢æå‡ºRynnECè§†é¢‘å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ”¯æŒåŒºåŸŸçº§è§†é¢‘äº¤äº’ï¼Œæå‡å…·èº«æ™ºèƒ½ä»»åŠ¡è¡¨ç°ã€‚


**ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models**

æå‡ºä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒèŒƒå¼ï¼Œæå‡å°å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚


### 3.4 å›¾åƒç”Ÿæˆ


> Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers


<details>
<summary>
Stage1: Tool-Driven Visual Exploration
-> Stage2: Programmatic Visual Manipulation
-> Stage3: Intrinsic Visual Imagination
</summary>

**Visual Planning: Let's Think Only with Images**
[![arXiv](https://img.shields.io/badge/arXiv-2505.11409-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2505.11409)   [![GitHub stars](https://img.shields.io/github/stars/yix8/VisualPlanning?style=social&label=GitHub&logo=github)](https://github.com/yix8/VisualPlanning)

> ![Visual Planning](assets/visual-planning-teaser.png)


**T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT**
[![arXiv](https://img.shields.io/badge/arXiv-2505.00703-b31b1b?logo=arxiv)](https://arxiv.org/abs/2505.00703)
[![Code](https://img.shields.io/github/stars/CaraJ7/T2I-R1?style=social&label=Code&logo=github)](https://github.com/CaraJ7/T2I-R1)

> Highlight: å°† R1 å¼é€æ­¥æ¨ç†/å¥–åŠ±æ€è·¯å¼•å…¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚
> 
> ![alt text](./assets/t2i-r1.png)

**GRIT: Teaching MLLMs to Think with Images**
[![WeChat](https://img.shields.io/badge/WeChat-Explainer-07C160?logo=wechat)](https://mp.weixin.qq.com/s/I3bk4FGOwrj-tKl_IYRfKw)

> Highlight: ä»¥è§†è§‰ä¸ºä¸­ä»‹çš„â€œå›¾åƒæ€è€ƒâ€èŒƒå¼è§£è¯»ä¸ç§‘æ™®ã€‚

**d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning (arXiv 2025)**

[![arXiv](https://img.shields.io/badge/arXiv-2504.12216-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2504.12216)

> Highlight: å°†å¼ºåŒ–å­¦ä¹ ç”¨äºæ‰©æ•£å¼å¤§æ¨¡å‹ï¼Œæå‡å¤æ‚æ¨ç†èƒ½åŠ›ã€‚

**DanceGRPO (Text-to-Image)**
[![Code](https://img.shields.io/github/stars/XueZeyue/DanceGRPO?style=social&label=Code&logo=github)](https://github.com/XueZeyue/DanceGRPO)

> Highlight: é¢å‘æ–‡ç”Ÿå›¾çš„ GRPO è®­ç»ƒä¸ç­–ç•¥å®ç°ã€‚

**Flow-GRPO: Training Flow Matching Models via Online RL**
[![Code](https://img.shields.io/github/stars/yifan123/flow_grpo?style=social&label=Code&logo=github)](https://github.com/yifan123/flow_grpo)

> Highlight: åŸºäº Flow/æ‰©æ•£æµç¨‹çš„ GRPO è®­ç»ƒèŒƒå¼ä¸å®è·µã€‚

**Interleaving Reasoning for Better Text-to-Image Generation (IRG)**
[![Code](https://img.shields.io/github/stars/Osilly/Interleaving-Reasoning-Generation?style=social&label=Code&logo=github)](https://github.com/Osilly/Interleaving-Reasoning-Generation)

> Highlight: äº¤é”™å¼æ¨ç†æå‡æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„ç»†èŠ‚ä¸ä¸€è‡´æ€§ï¼›æä¾›å¯å¤ç°å®éªŒä¸å¼€æºä»£ç ã€‚


**Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination (DIM)**
[![Code](https://img.shields.io/github/stars/showlab/DIM?style=social&label=Code&logo=github)](https://github.com/showlab/DIM)

> Highlight: é“¾å¼æ€ç»´é©±åŠ¨çš„ç²¾ç¡®å›¾åƒç¼–è¾‘ä¸ä¸“ç”¨æ•°æ®é›†ï¼Œæ˜¾è‘—æå‡å¤æ‚æŒ‡ä»¤ä¸‹çš„ç¼–è¾‘ç²¾åº¦ã€‚

**PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting**
[![arXiv](https://img.shields.io/badge/arXiv-2509.04545-b31b1b?logo=arxiv)](https://arxiv.org/abs/2509.04545)
[![Code](https://img.shields.io/github/stars/Hunyuan-PromptEnhancer/PromptEnhancer?style=social&label=Code&logo=github)](https://github.com/Hunyuan-PromptEnhancer/PromptEnhancer)
[![Project](https://img.shields.io/badge/Project-Website-0a84ff?logo=safari)](https://hunyuan-promptenhancer.github.io/)

> Highlight: åˆ©ç”¨æ€ç»´é“¾å¯¹æç¤ºè¯­è¿›è¡Œé‡å†™ï¼Œæ”¹å–„æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆè´¨é‡ä¸å¯¹é½ç¨‹åº¦ã€‚

</details>

### 3.5 GUI ä»£ç†ä¸å±å¹•æ“ä½œ

ä¿¡æ¯ç©ºé—´äº¤äº’ï¼Œè¿ˆå‘å…·èº«æ™ºèƒ½

UI-R1 â€” å›¾å½¢ç•Œé¢æ™ºèƒ½ä½“åŠ¨ä½œé¢„æµ‹ [é¡¹ç›®](https://github.com/vivo-ai-lab/UI-R1)
Qwen-Agent â€” å·¥å…·å¢å¼ºä¸GUIè‡ªåŠ¨åŒ–ç”Ÿæ€ [é¡¹ç›®](https://github.com/QwenLM/Qwen-Agent/tree/main)


Mobile-Agent-v3: Fundamental Agents for GUI Automation
https://arxiv.org/pdf/2508.15144
https://github.com/X-PLUG/MobileAgent

![mobileagentv3_framework](./assets/mobileagentv3-framework.png)


Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding
**ä¸»è¦å†…å®¹**ï¼šæå‡ºLASERæ¡†æ¶ï¼Œæå‡VLMåœ¨GUIç†è§£ä»»åŠ¡ä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥ä¸å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚
**é“¾æ¥**ï¼š[https://github.com/wwfnb/Laser](https://github.com/wwfnb/Laser)




## 4. ğŸ¤– Thinking with Embodiment

ä»è¯­è¨€æ€è€ƒåˆ°å›¾æ–‡æ€è€ƒï¼Œæˆ‘ä»¬é€æ­¥å¢å¼ºäº†æ¨¡å‹åœ¨æ•°å­—ä¸–ç•Œä¸­çš„æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¦å®ç°é€šç”¨äººå·¥æ™ºèƒ½ï¼Œæ¨¡å‹å¿…é¡»èƒ½å¤Ÿä¸ç‰©ç†ä¸–ç•Œè¿›è¡Œäº¤äº’ã€‚æœ¬ç« å°†æ¢è®¨â€œæ€è€ƒâ€çš„æœ€ç»ˆå‰æ²¿ï¼š**å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰**ã€‚

åœ¨è¿™é‡Œï¼Œâ€œæ€è€ƒâ€ä¸å†ä»…ä»…æ˜¯ä¸ºäº†ç†è§£æˆ–ç”Ÿæˆå†…å®¹ï¼Œè€Œæ˜¯ä¸ºäº†**æŒ‡å¯¼è¡ŒåŠ¨**ã€‚æ¨¡å‹éœ€è¦å°†æ„ŸçŸ¥ã€æ¨ç†ä¸ç‰©ç†åŠ¨ä½œï¼ˆå¦‚å¯¼èˆªã€æŠ“å–ã€æ“ä½œï¼‰ç›¸ç»“åˆï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„â€œæ„ŸçŸ¥â€”æ€è€ƒâ€”è¡ŒåŠ¨â€é—­ç¯ã€‚æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰å¦‚ä½•æˆä¸ºè¿æ¥æ•°å­—æ™ºèƒ½ä¸ç‰©ç†ç°å®çš„å…³é”®æ¡¥æ¢ã€‚

### 4.1 VLA is All You Need ï¼

**Magma: A Foundation Model for Multimodal AI Agents (CVPR 2025)**
[![arXiv](https://img.shields.io/badge/arXiv-2502.13130-b31b1b?logo=arxiv)](https://www.arxiv.org/pdf/2502.13130)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://microsoft.github.io/Magma/)
[![GitHub stars](https://img.shields.io/github/stars/microsoft/Magma?style=social&label=GitHub&logo=github)](https://github.com/microsoft/Magma)


Microsoft Research é«˜å‰‘å³°å›¢é˜Ÿ

![alt text](./assets/magma-overview.png)


F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions

![alt text](./assets/f1-paradigms-for-manipulation-policies.png)


RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics

[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://robo-point.github.io/)
[![GitHub stars](https://img.shields.io/github/stars/wentaoyuan/RoboPoint?style=social&label=GitHub&logo=github)](https://github.com/wentaoyuan/RoboPoint)
[![Datasets](https://img.shields.io/badge/Datasets-HuggingFace-orange?logo=huggingface)](https://huggingface.co/datasets/wentao-yuan/robopoint-data)

![Embodied-Reasoner](assets/robopoint.png)

From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation

[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://embodied-fsd.github.io/)
[![GitHub stars](https://img.shields.io/github/stars/pickxiguapi/Embodied-FSD?style=social&label=GitHub&logo=github)](https://github.com/pickxiguapi/Embodied-FSD)
[![Datasets](https://img.shields.io/badge/Datasets-HuggingFace-orange?logo=huggingface)](https://huggingface.co/IffYuan)
[![Email](https://img.shields.io/badge/Contact-yuanyf%40tju.edu.cn-informational?logo=gmail)](mailto:yuanyf@tju.edu.cn)

![Embodied-Reasoner](assets/fsd.png)

Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation

[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://embodied-r1.github.io/)
[![GitHub stars](https://img.shields.io/github/stars/pickxiguapi/Embodied-R1?style=social&label=GitHub&logo=github)](https://github.com/pickxiguapi/Embodied-R1)
[![Datasets](https://img.shields.io/badge/Datasets-HuggingFace-orange?logo=huggingface)](https://huggingface.co/IffYuan)
[![Email](https://img.shields.io/badge/Contact-yuanyf%40tju.edu.cn-informational?logo=gmail)](mailto:yuanyf@tju.edu.cn)

![Embodied-Reasoner](assets/embodied-r1.png)

Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks

[![arXiv](https://img.shields.io/badge/arXiv-2503.21696-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2503.21696v1)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://embodied-reasoner.github.io/)
[![GitHub stars](https://img.shields.io/github/stars/zwq2018/embodied_reasoner?style=social&label=GitHub&logo=github)](https://github.com/zwq2018/embodied_reasoner)

![Embodied-Reasoner](assets/embodied-reasoner-overview.png)

---

Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning

[![arXiv](https://img.shields.io/badge/arXiv-2503.20752-b31b1b?logo=arxiv)](https://arxiv.org/abs/2503.20752)
[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://tanhuajie.github.io/ReasonRFT/)
[![GitHub stars](https://img.shields.io/github/stars/tanhuajie/Reason-RFT?style=social&label=GitHub&logo=github)](https://github.com/tanhuajie/Reason-RFT)

![Reason-RFT](assets/teaser.png)

---

Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation

[![CVPR 2025](https://img.shields.io/badge/CVPR-2025-blue)](https://openaccess.thecvf.com/content/CVPR2025/papers/Yao_Think_Small_Act_Big_Primitive_Prompt_Learning_for_Lifelong_Robot_CVPR_2025_paper.pdf)

![Think Small, Act Big](assets/think-small-act-big.png)

---

### 4.2 å…·èº«å¯¼èˆªï¼ˆæ— äººæœºï¼‰
OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation

[![Project](https://img.shields.io/badge/Project-blue?logo=safari)](https://shailab-ipec.github.io/openfly/)

![OpenFly](assets/toolchain.png)

---



AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving

[![arXiv](https://img.shields.io/badge/arXiv-2505.15298-b31b1b?logo=arxiv)](https://arxiv.org/pdf/2505.15298)

![AgentThink](assets/agentthink-overview.png)
å°ç±³å›¢é˜Ÿ



---

å‰é¢æˆ‘ä»¬æ¢è®¨äº†â€œæ€è€ƒâ€åœ¨ä¸åŒæ¨¡æ€ï¼ˆè¯­è¨€ã€å›¾åƒã€å…·èº«ï¼‰ä¸­çš„ç†è®ºä¸åº”ç”¨ã€‚ä¸ºäº†å°†è¿™äº›å‰æ²¿ç†å¿µä»˜è¯¸å®è·µï¼Œæˆ‘ä»¬éœ€è¦å¼ºå¤§çš„å·¥å…·å’Œæ¡†æ¶ã€‚æœ¬ç« èŠ‚å°†èšç„¦äºå®ç°è¿™äº›å¤æ‚ç³»ç»Ÿæ‰€éœ€çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯å¼ºåŒ–å­¦ä¹ ç›¸å…³çš„æ¡†æ¶ä¸ç®—æ³•ï¼Œå¹¶æä¾›ä¸€ç³»åˆ—å¯ä»¥ç›´æ¥ä¸Šæ‰‹çš„å¼€æºé¡¹ç›®ã€‚

## 5. ğŸ› ï¸ Tutorials and Tooling


### 5.1 å¼ºåŒ–å­¦ä¹ æ¡†æ¶

å¾®è½¯ç ”ç©¶é™¢æå‡ºçš„é›¶ä»£ç å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ– AI Agent çš„å·¥ä¸šçº§æ–¹æ¡ˆã€‚
https://github.com/microsoft/agent-lightning

### 5.2 å¼ºåŒ–å­¦ä¹ ç®—æ³•

**Title**: Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning
**ä¸»è¦å†…å®¹**: é˜¿é‡Œå›¢é˜Ÿå…³äºRL4LLMé¢†åŸŸçš„è®ºæ–‡ï¼Œåˆ†æPPOç­‰RLæ–¹æ³•åœ¨å¤§æ¨¡å‹æ¨ç†ä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜ã€‚


å¡ç‰‡åŒ–æ±‡æ€»ä¸»æµ/æ–°è¿‘çš„å¯¹é½ä¸ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œç»Ÿä¸€å±•ç¤º arXiv / å®˜æ–¹é“¾æ¥ / ä»£ç ã€‚

DPO: Direct Preference Optimization (arXiv 2023)
[![arXiv](https://img.shields.io/badge/arXiv-2305.18290-b31b1b?logo=arxiv)](https://arxiv.org/abs/2305.18290)

Highlight: ç›´æ¥åœ¨åå¥½æ•°æ®ä¸Šæ‹Ÿåˆæ¦‚ç‡ååºï¼Œé¿å…æ˜¾å¼å¥–åŠ±å»ºæ¨¡ã€‚

Agentic Reinforced Policy Optimization (ARPO, arXiv 2025)
[![arXiv](https://img.shields.io/badge/arXiv-2507.19849-b31b1b?logo=arxiv)](https://arxiv.org/abs/2507.19849)

Highlight: é¢å‘æ™ºèƒ½ä½“äº¤äº’çš„å¼ºåŒ–å¯¹é½ç­–ç•¥ä¼˜åŒ–ï¼Œèšç„¦å¤šé˜¶æ®µæ¨ç†è´¨é‡ã€‚

GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization (ACL 2025)
[![ACL](https://img.shields.io/badge/ACL-2025-1877F2)](https://aclanthology.org/2025.acl-long.13/) [![Code](https://img.shields.io/github/stars/MikeGu721/GAPO?style=social&label=Code&logo=github)](https://github.com/MikeGu721/GAPO)

Highlight: ç”Ÿæˆå¼å¯¹æŠ— + åå¥½ä¿¡å·è”åˆï¼Œæå‡æç¤ºå­¦ä¹ ä¸ç­–ç•¥ç¨³å®šæ€§ã€‚

PAPO: Perception-Aware Policy Optimization for Multimodal Reasoning
[![Code](https://img.shields.io/github/stars/MikeWangWZHL/PAPO?style=social&label=Code&logo=github)](https://github.com/MikeWangWZHL/PAPO)

Highlight: é’ˆå¯¹ GRPO çš„æ”¹è¿›æ–¹æ³•ï¼ˆç¨³å®šæ€§ / æ”¶æ•›æ•ˆç‡ï¼‰ã€‚

TreePO: Bridging Policy Optimization and Inference Efficiency (arXiv 2025)
[![arXiv](https://img.shields.io/badge/arXiv-2508.17445-b31b1b?logo=arxiv)](https://arxiv.org/abs/2508.17445)

Highlight: å¼•å…¥æ ‘å¼å¯å‘ç»“æ„ï¼Œå…¼é¡¾å¯¹é½æ•ˆæœä¸æ¨ç†æ•ˆç‡ã€‚

RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling (arXiv 2025)
[![arXiv](https://img.shields.io/badge/arXiv-2509.06461v1-b31b1b?logo=arxiv)](https://arxiv.org/abs/2509.06461v1) [![Code](https://img.shields.io/github/stars/bigai-nlco/RuleReasoner?style=social&label=Code&logo=github)](https://github.com/bigai-nlco/RuleReasoner)

Highlight: ç»“åˆé¢†åŸŸè§„åˆ™åŠ¨æ€é‡‡æ ·ä¸å¼ºåŒ–ä¼˜åŒ–ï¼Œæé«˜ç»“æ„åŒ–æ¨ç†è´¨é‡ä¸å¯æ§æ€§ã€‚


PREF-GRPO: Pairwise Preference Reward-Based GRPO for Stable Text-to-Image Reinforcement Learning
- **ä½œè€…**ï¼šYibin Wang, Zhimin Li, Yuhang Zang, ç­‰
- **ä¸»è¦å†…å®¹**ï¼šæå‡ºé¦–ä¸ªåŸºäºæˆå¯¹åå¥½å¥–åŠ±çš„GRPOæ–¹æ³•ï¼Œæå‡æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„ç¨³å®šæ€§ï¼Œç¼“è§£reward hackingé—®é¢˜ã€‚
- **é“¾æ¥**ï¼š[codegoat24.github.io/UnifiedReward/Pref-GRPO](https://codegoat24.github.io/UnifiedReward/Pref-GRPO)
- **arXiv**ï¼š[arxiv.org/pdf/2508.20751](https://arxiv.org/pdf/2508.20751)

---
### 5.3 æ“ä½œå®ç°


[![EasyR1](https://img.shields.io/github/stars/hiyouga/EasyR1?style=social&label=EasyR1&logo=github)](https://github.com/hiyouga/EasyR1) â€” R1 è®­ç»ƒä¸å¤ç°çš„ç®€æ´æ¨¡æ¿ã€‚


[![Multimodal-Open-R1](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal?style=social&label=Multimodal-Open-R1&logo=github)](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal) â€” é€šç”¨å¤šæ¨¡æ€ R1ã€‚


[![R1-VL](https://img.shields.io/github/stars/jingyi0000/R1-VL?style=social&label=R1-VL&logo=github)](https://github.com/jingyi0000/R1-VL) â€” è§†è§‰-è¯­è¨€é€æ­¥å¥–åŠ±ã€‚

[![Open-R1-Video](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video?style=social&label=Open-R1-Video&logo=github)](https://github.com/Wang-Xiaodong1899/Open-R1-Video) â€” å¼€æºè§†é¢‘ R1 èŒƒå¼ã€‚


[![RLFactory-Agent](https://img.shields.io/github/stars/Simple-Efficient/RL-Factory?style=social&label=RLFactory-Agent&logo=github)](https://github.com/Simple-Efficient/RL-Factory) â€” RL å·¥å…·ä¸ä»£ç†è®­ç»ƒæ¡†æ¶ã€‚

[![Qwen-Agent](https://img.shields.io/badge/Industry-Qwen--Agent-blueviolet?logo=github)](https://github.com/QwenLM/Qwen-Agent/tree/main)


## 6. ğŸ“– Related Collections

[![arXiv](https://img.shields.io/badge/arXiv-2508.17281-b31b1b?logo=arxiv)](https://arxiv.org/abs/2508.17281) â€” From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users 


[![System-2-Research](https://img.shields.io/github/stars/open-thought/system-2-research?style=flat&color=black&label=system-2-research&logo=github&logoColor=white)](https://github.com/open-thought/system-2-research) â€” Systemâ€‘2 agents, toolâ€‘use, and planning resources.

[![Efficient-Reasoning](https://img.shields.io/github/stars/Zanette-Labs/efficient-reasoning?style=flat&color=black&label=efficient-reasoning&logo=github&logoColor=white)](https://github.com/Zanette-Labs/efficient-reasoning) â€” Compute/dataâ€‘efficient reasoning methods and benchmarks.


[![Awesome-Thinking-With-Images](https://img.shields.io/github/stars/ligeng0197/Awesome-Thinking-With-Images?style=flat&color=black&label=Awesome-Thinking-With-Images&logo=github&logoColor=white)](https://github.com/ligeng0197/Awesome-Thinking-With-Images) â€” Broad visual thinking and perception resources.

[![Thinking-with-Generated-Images](https://img.shields.io/github/stars/GAIR-NLP/thinking-with-generated-images?style=flat&color=black&label=thinking-with-generated-images&logo=github&logoColor=white)](https://github.com/GAIR-NLP/thinking-with-generated-images) â€” Reasoning with generated intermediate images (papers + code).


[![Awesome-Self-Evolving-Agents](https://img.shields.io/github/stars/EvoAgentX/Awesome-Self-Evolving-Agents?style=flat&color=black&label=Awesome-Self-Evolving-Agents&logo=github&logoColor=white)](https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents) â€” Selfâ€‘evolving agents (autoâ€‘improvement, reflection, curricula).


### 6.1 ç»¼è¿°æ–‡ç« 


[![Awesome_Think_With_Images](https://img.shields.io/github/stars/zhaochen0110/Awesome_Think_With_Images?style=flat&color=black&label=Awesome_Think_With_Images&logo=github&logoColor=white)](https://github.com/zhaochen0110/Awesome_Think_With_Images) â€” Visual-only reasoning with images (papers + code).


Agent AI: Surveying the Horizons of Multimodal Interaction
æé£é£å›¢é˜Ÿ

AI agentï¼ˆAI æ™ºèƒ½ä½“ï¼‰å®šä¹‰ï¼šä¸€ä¸ªå…·ä½“çš„ç³»ç»Ÿæˆ–ç¨‹åºï¼Œå®ƒèƒ½å¤Ÿåœ¨æŸä¸ªç¯å¢ƒä¸­æ„ŸçŸ¥ã€å†³ç­–å¹¶é‡‡å–è¡ŒåŠ¨ä»¥å®Œæˆä»»åŠ¡ã€‚
Agent AI å®šä¹‰ï¼šä¸€ç±»ä»¥æ™ºèƒ½ä½“ä¸ºæ ¸å¿ƒèŒƒå¼çš„äººå·¥æ™ºèƒ½æŠ€æœ¯æˆ–ç³»ç»Ÿæ¡†æ¶ï¼Œå…³æ³¨å¤šæ¨¡æ€äº¤äº’ã€åŠ¨æ€é€‚åº”èƒ½åŠ›ä»¥åŠé€šç”¨æ™ºèƒ½çš„å®ç°ã€‚
AI agent æ˜¯å®ç°ï¼ŒAgent AI æ˜¯æ¡†æ¶å’Œç ”ç©¶æ–¹å‘


Explain Before You Answer: A Survey on Compositional Visual Reasoning
- **ä½œè€…**ï¼šFucai Ke, Joy Hsu, Zhixi Cai, ç­‰
- **ä¸»è¦å†…å®¹**ï¼šç»¼è¿°2023-2025å¹´ç»„åˆå¼è§†è§‰æ¨ç†ï¼ˆCVRï¼‰ç›¸å…³æ–‡çŒ®ï¼Œç³»ç»Ÿæ¢³ç†260+è®ºæ–‡ã€‚
- **arXiv**ï¼š[arXiv:2508.17298](https://arxiv.org/abs/2508.17298)



**The Landscape of Agentic Reinforcement Learning for LLMs: A Survey**
[![arXiv](https://img.shields.io/badge/arXiv-2509.02547-b31b1b?logo=arxiv)](https://arxiv.org/abs/2509.02547) 
[![Awesome-AgenticLLM-RL-Papers](https://img.shields.io/github/stars/xhyumiracle/Awesome-AgenticLLM-RL-Papers?style=flat&color=black&label=Awesome-AgenticLLM-RL-Papers&logo=github&logoColor=white)](https://github.com/xhyumiracle/Awesome-AgenticLLM-RL-Papers) 



**The Landscape of Agentic Reinforcement Learning for LLMs: A Survey.**


**A Survey of Reinforcement Learning for Large Reasoning Models**
åœ°å€ï¼šhttps://arxiv.org/pdf/2509.08827
å‘¨ä¼¯æ–‡è€å¸ˆå›¢é˜Ÿ

![aaa](./assets/survey_rl4lrm.png)

**ä¸»è¦å†…å®¹**: æ¸…åå¤§å­¦ç­‰å›¢é˜Ÿç³»ç»Ÿæ¢³ç†äº†RL+LLMsé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œæ¶µç›–å¥–åŠ±è®¾è®¡ã€è®­ç»ƒèŒƒå¼ã€å¤šæ™ºèƒ½ä½“ã€å·¥å…·å­¦ä¹ ç­‰å¤šä¸ªæ–¹å‘ã€‚
**é“¾æ¥**: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs